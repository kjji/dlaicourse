{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_ChatBot_using_seq2seq_LSTM_Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjji/dlaicourse/blob/master/5_ChatBot_using_seq2seq_LSTM_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BhwN0XQX4Icu"
      },
      "source": [
        "# Chatbot using Seq2Seq LSTM models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tm5g4WIG5ym2"
      },
      "source": [
        "## Importing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UgZHR8TO0lFF",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers , activations , models , preprocessing\n",
        "\n",
        "print( tf.VERSION )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "imkdw4os6FI4"
      },
      "source": [
        "## Download the preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6i6u8US30ufe",
        "colab": {}
      },
      "source": [
        "import requests, zipfile, io\n",
        "\n",
        "r = requests.get( 'https://github.com/shubham0204/Dataset_Archives/blob/master/chatbot_nlp_processed.zip?raw=true' ) \n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1FVS7OZK7LwF"
      },
      "source": [
        "## Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UPODnE9T07bd",
        "colab": {}
      },
      "source": [
        "encoder_input_data = np.load( 'chatbot_nlp_processed/data/enc_in_data.npy' )\n",
        "decoder_input_data = np.load( 'chatbot_nlp_processed/data/dec_in_data.npy' )\n",
        "decoder_target_data = np.load( 'chatbot_nlp_processed/data/dec_tar_data.npy' )\n",
        "\n",
        "embedding_matrix = np.load('chatbot_nlp_processed/data/embedding.npy' ) \n",
        "\n",
        "tokenizer = pickle.load( open('chatbot_nlp_processed/data/tokenizer.pkl' , 'rb'))\n",
        "\n",
        "num_tokens = len( tokenizer.word_index )+1\n",
        "word_dict = tokenizer.word_index\n",
        "\n",
        "max_question_len = encoder_input_data.shape[1]\n",
        "max_answer_len = decoder_input_data.shape[1]\n",
        "\n",
        "print( 'Max length of question is {}'.format( max_question_len) )\n",
        "print( 'Max length of answer is {}'.format( max_answer_len) )\n",
        "\n",
        "print( encoder_input_data.shape )\n",
        "print( decoder_input_data.shape )\n",
        "print( decoder_target_data.shape )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4SwY3T139l19"
      },
      "source": [
        "## Defining the Encoder-Decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gUYtOwv21rt",
        "colab": {}
      },
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( num_tokens, 200 , mask_zero=True , weights=[embedding_matrix] ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( num_tokens, 200 , mask_zero=True, weights=[embedding_matrix]) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( num_tokens , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n9g_8sR7WWf3"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N74NZnfo3Id-",
        "colab": {}
      },
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=50, epochs=50 ) \n",
        "model.save( 'model.h5' ) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3sOLQr0M-lAe"
      },
      "source": [
        "## Defining inference models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1u5DE4qo3Mf2",
        "colab": {}
      },
      "source": [
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rxZp0ZRy-6dA"
      },
      "source": [
        "## Talking with our Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5P_wDD554q9O",
        "colab": {}
      },
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( word_dict[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_question_len , padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2zBmN8qB3O-e",
        "colab": {}
      },
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = word_dict['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in word_dict.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > max_answer_len:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHZvdETMSA0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}