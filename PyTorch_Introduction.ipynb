{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Introduction by LeadingIndia.ai .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjji/dlaicourse/blob/master/PyTorch_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs3raTEsRISM",
        "colab_type": "text"
      },
      "source": [
        "<center>\n",
        "\n",
        "</center>\n",
        "\n",
        "# 1. Introduction\n",
        "\n",
        "\n",
        "*   PyTorch is a deep learning research platform that provides maximum flexibility and speed\n",
        "*   It was developed by **Facebook** and is based on now obsolete Torch\n",
        "* PyTorch also provides tensor computation (like NumPy) with strong GPU acceleration\n",
        "* PyTorch deals extensively in Tensors\n",
        "* Tensors act as <font color=red><b>fundamental data structure</b></font>\n",
        "* PyTorch Documentation - <b>https://pytorch.org/docs/stable/index.html</b>\n",
        "\n",
        "\n",
        "\n",
        "Importing and checking version of PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0tUjAj8Rmez",
        "colab_type": "code",
        "outputId": "cc1d8547-072c-4ade-d781-6e4e0caa3e2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch #to import PyTorch\n",
        "print('PyTorch version:', torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version: 1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "902_ZwoYrXAq",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Understanding Tensors in PyTorch\n",
        "\n",
        "\n",
        "\n",
        "*   PyTorch Tensors behave very similar to NumPy Arrays\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Creating a random tensor of m*n dimensions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olaj31H2qx6W",
        "colab_type": "code",
        "outputId": "e6dcba99-fdb9-4c7a-9f6f-737b512872e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "x=torch.rand(5,4)#m=5,n=6\n",
        "print(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0165, 0.6706, 0.0081, 0.7907],\n",
            "        [0.3654, 0.4306, 0.9404, 0.8168],\n",
            "        [0.5222, 0.6452, 0.1185, 0.9221],\n",
            "        [0.1288, 0.1981, 0.5690, 0.6710],\n",
            "        [0.8943, 0.7980, 0.9445, 0.5872]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS1hoz0sr4mQ",
        "colab_type": "text"
      },
      "source": [
        "**To find size of the tensor use size() function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACQB24LDro_B",
        "colab_type": "code",
        "outputId": "41604a09-eb65-48ae-8f85-c6028233da89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x.size())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWKpL_I0s8YF",
        "colab_type": "text"
      },
      "source": [
        "**Creating a Tensor consisting of ones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4-ydFQ5r4Nm",
        "colab_type": "code",
        "outputId": "c406a250-4f38-4ec0-8732-0176830fb9e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "y=torch.ones(5,4)\n",
        "print(y)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSlQw9o7tWgD",
        "colab_type": "text"
      },
      "source": [
        "**Adding two Tensors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns9K0MDJtA1l",
        "colab_type": "code",
        "outputId": "5ac1290e-011f-4360-c4ea-0eac13cfc058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "z=x+y\n",
        "print(z)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.0165, 1.6706, 1.0081, 1.7907],\n",
            "        [1.3654, 1.4306, 1.9404, 1.8168],\n",
            "        [1.5222, 1.6452, 1.1185, 1.9221],\n",
            "        [1.1288, 1.1981, 1.5690, 1.6710],\n",
            "        [1.8943, 1.7980, 1.9445, 1.5872]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TNX-GAIttJu",
        "colab_type": "text"
      },
      "source": [
        "**Getting Rows and Columns of Tensors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er2NvsCctZtR",
        "colab_type": "code",
        "outputId": "3ef60c4b-8061-49ee-8bb2-49236a217bc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(x[1])#to get first row of Tensor x\n",
        "print(x[:,2])#to get  2nd column of Tensor x"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.3654, 0.4306, 0.9404, 0.8168])\n",
            "tensor([0.0081, 0.9404, 0.1185, 0.5690, 0.9445])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gy2wVO7ihBD",
        "colab_type": "text"
      },
      "source": [
        "**There are two types of methods associated with Tensors:**\n",
        "\n",
        "\n",
        "\n",
        "1.   Methods returning a copy and not modifying original Tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnhzHNh_imjf",
        "colab_type": "code",
        "outputId": "01de9c3e-509f-4784-f75d-2c1ff321d1ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "x.add(1)#will not modify original Tensor"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0165, 1.6706, 1.0081, 1.7907],\n",
              "        [1.3654, 1.4306, 1.9404, 1.8168],\n",
              "        [1.5222, 1.6452, 1.1185, 1.9221],\n",
              "        [1.1288, 1.1981, 1.5690, 1.6710],\n",
              "        [1.8943, 1.7980, 1.9445, 1.5872]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5XKWOCNi-x4",
        "colab_type": "code",
        "outputId": "ad3274c0-716e-46d2-9006-3768c53a583c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0165, 0.6706, 0.0081, 0.7907],\n",
            "        [0.3654, 0.4306, 0.9404, 0.8168],\n",
            "        [0.5222, 0.6452, 0.1185, 0.9221],\n",
            "        [0.1288, 0.1981, 0.5690, 0.6710],\n",
            "        [0.8943, 0.7980, 0.9445, 0.5872]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p1WOWTPjJAN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "2.   In place version of methods that modify the original Tensor (normally ends with _)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3s0tegBjFBL",
        "colab_type": "code",
        "outputId": "9b50bd91-a655-4f12-962f-1d77c11d8fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "x.add_(1)#will modify original Tensor x"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0165, 1.6706, 1.0081, 1.7907],\n",
              "        [1.3654, 1.4306, 1.9404, 1.8168],\n",
              "        [1.5222, 1.6452, 1.1185, 1.9221],\n",
              "        [1.1288, 1.1981, 1.5690, 1.6710],\n",
              "        [1.8943, 1.7980, 1.9445, 1.5872]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufRQHeisjRIL",
        "colab_type": "code",
        "outputId": "6c46aa69-74f8-45a5-feaa-fdc762fb5bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.0165, 1.6706, 1.0081, 1.7907],\n",
            "        [1.3654, 1.4306, 1.9404, 1.8168],\n",
            "        [1.5222, 1.6452, 1.1185, 1.9221],\n",
            "        [1.1288, 1.1981, 1.5690, 1.6710],\n",
            "        [1.8943, 1.7980, 1.9445, 1.5872]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts72e3lAj2Xv",
        "colab_type": "text"
      },
      "source": [
        "**Getting shape and reshaping the Tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI35UPm9jZHf",
        "colab_type": "code",
        "outputId": "24bc35bc-c610-46dd-fc91-a8af638b8c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print(x)\n",
        "print(x.size())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.0165, 1.6706, 1.0081, 1.7907],\n",
            "        [1.3654, 1.4306, 1.9404, 1.8168],\n",
            "        [1.5222, 1.6452, 1.1185, 1.9221],\n",
            "        [1.1288, 1.1981, 1.5690, 1.6710],\n",
            "        [1.8943, 1.7980, 1.9445, 1.5872]])\n",
            "torch.Size([5, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Zejq7yj-E1",
        "colab_type": "code",
        "outputId": "5abec2df-7147-479e-8c3a-564fad3d9c54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "x.resize_(4,5)#reshaping Tensor to 4*5\n",
        "print(x)\n",
        "print(x.size())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.0165, 1.6706, 1.0081, 1.7907, 1.3654],\n",
            "        [1.4306, 1.9404, 1.8168, 1.5222, 1.6452],\n",
            "        [1.1185, 1.9221, 1.1288, 1.1981, 1.5690],\n",
            "        [1.6710, 1.8943, 1.7980, 1.9445, 1.5872]])\n",
            "torch.Size([4, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg4fOcbrlkqv",
        "colab_type": "text"
      },
      "source": [
        "**Converting a NumPy array to PyTorch Tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojuMkjFGkHdU",
        "colab_type": "code",
        "outputId": "33aa226a-7f5a-474a-df57-cf608859c013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import numpy as np\n",
        "a = np.random.rand(3,4)#creating a random numpy array of dimension 3*4\n",
        "print(a)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.41351569 0.85557927 0.63749808 0.50511222]\n",
            " [0.04835039 0.73102466 0.79719397 0.2480437 ]\n",
            " [0.27706977 0.05149605 0.46933624 0.47214787]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBdWElhTmEP0",
        "colab_type": "code",
        "outputId": "fdc4dcfd-dff9-4962-bf71-d5ab4c57c419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "b = torch.from_numpy(a)#converting numpy array into Torch tensor\n",
        "print(b)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.4135, 0.8556, 0.6375, 0.5051],\n",
            "        [0.0484, 0.7310, 0.7972, 0.2480],\n",
            "        [0.2771, 0.0515, 0.4693, 0.4721]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td676isEmpQg",
        "colab_type": "text"
      },
      "source": [
        "**Convering Torch Tensor to Numpy array**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJUSyc5Hmgck",
        "colab_type": "code",
        "outputId": "4857d732-2fd6-4631-d20a-e5b410179d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "c = b.numpy()\n",
        "print(type(c))\n",
        "print(c)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[[0.41351569 0.85557927 0.63749808 0.50511222]\n",
            " [0.04835039 0.73102466 0.79719397 0.2480437 ]\n",
            " [0.27706977 0.05149605 0.46933624 0.47214787]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZldRyt6BnO2M",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>**Note :** </font>NumPy array and PyTorch tensors share memory. Thus if one is modifies other will automatically get updated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuLz-uhQnACL",
        "colab_type": "code",
        "outputId": "5b38b773-c14d-4574-b16c-812f0c185737",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "b[0,1] = 9 #modifying tensor b\n",
        "print(b)\n",
        "print(c) #NumPy array c which was created from PyTorch tensor b will automatically get updated"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.4135, 9.0000, 0.6375, 0.5051],\n",
            "        [0.0484, 0.7310, 0.7972, 0.2480],\n",
            "        [0.2771, 0.0515, 0.4693, 0.4721]], dtype=torch.float64)\n",
            "[[0.41351569 9.         0.63749808 0.50511222]\n",
            " [0.04835039 0.73102466 0.79719397 0.2480437 ]\n",
            " [0.27706977 0.05149605 0.46933624 0.47214787]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AhFkj70oMRg",
        "colab_type": "text"
      },
      "source": [
        "# 2. Deep Learning using PyTorch\n",
        "\n",
        "\n",
        "*   We will be using **[TORCHVISION](https://pytorch.org/docs/stable/torchvision/index.html)** which contains many of popular dataset readily available to use in PyTorch\n",
        "* As an introduction we will start with **[MNIST](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist)** Dataset\n",
        "\n",
        "## 2.1 Building the Neural Network\n",
        "**Importing the necessary packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PABioUvmnt-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch #base library\n",
        "\n",
        "import helper #to visually analyze the output\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms #for importing datasets and applying some transformations on those datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxzHqMQpslWd",
        "colab_type": "text"
      },
      "source": [
        "**Downloading, tranforming and splitting the MNIST dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqMsSB7WsvNl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "f0302862-a05a-4f4a-f364-8ab14d7dff38"
      },
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5, ), (0.5,)),# Subtract 0.5 from each pixel value and then divide it by 0.5 so as convert range of pixel value form -1 to +1\n",
        "                             ])\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8970802.96it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 130154.22it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2137018.74it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 49704.52it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdtuYvyeuVwZ",
        "colab_type": "text"
      },
      "source": [
        "Goto Files by selecting left menu <font size=5> ▶️</font> and select refresh and then ensure that MNIST dataset is actually saved on the disk\n",
        "\n",
        "**Ensuring that dataset has loaded correctly**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIXLF3bNuO8-",
        "colab_type": "code",
        "outputId": "e9c5fc3d-3937-457a-a14b-d03336b50e3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "plt.imshow(images[8].numpy().squeeze(),cmap='Greys_r')#display the 8th image in the batch\n",
        "print(labels[8].numpy())#display the label of 8th image in the batch"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCD8QFOEHgvoLtOlOwgCEg6MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r718y5M98tn6",
        "colab_type": "text"
      },
      "source": [
        "**Details of the network that we will be building**\n",
        "\n",
        "\n",
        "\n",
        "*   Input Layer 784 units\n",
        "*   Hidden Layer 1 with 128 units having ReLU activation\n",
        "*  Hidden Layer 2 with 64 units having ReLU activation\n",
        "* Output Layer with 10 units having Softmax activation for classification of digits\n",
        "* The Loss function will be cross entropy loss\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPqupfEOwzS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn #to build neural network\n",
        "from torch import optim #for optimization algorithms like ADAM, RMSProp\n",
        "import torch.nn.functional as F #many neural network functions such as relu, softmax, convolution etc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAAV2aIJA9zu",
        "colab_type": "text"
      },
      "source": [
        "**To create Deep Neural Networks, generally we have to create a python class that is subclass of nn.Module**\n",
        "\n",
        "We have two options:\n",
        "\n",
        "\n",
        "1.   Create a subclass of nn.Module and specify the network architecture\n",
        "2.   Using an auxillary module nn.Sequential() to build a model where tensor will be passed from one layer to other sequentially\n",
        "\n",
        "We can use either one of them.\n",
        "\n",
        "<font color=red>**Option 1:**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaiIBbov_Tlq",
        "colab_type": "code",
        "outputId": "7ae24e9a-e9d5-47a9-f781-212dc63f1ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "class Network(nn.Module):#specifying our network architecture\n",
        "  def __init__(self): #initializing the network\n",
        "    super().__init__()#it will call the init method of nn.Module\n",
        "    self.fc1 = nn.Linear(784,128)\n",
        "    self.fc2 = nn.Linear(128,64)\n",
        "    self.fc3 = nn.Linear(64,10)\n",
        "  \n",
        "  #all PyTorch Network must have forward method representing forward pass\n",
        "  def forward(self,x):#x is a tensor passed to forward function\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.softmax(x, dim=1)#The tensor will be of size 64(batcg size) * 10; we want to apply softmax to secomd dimension\n",
        "    return x   \n",
        "model = Network()\n",
        "model"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd9GmCr2gk_M",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>**Option 2:**</font> **Creating a Sequential Model to pass tensors from one layer to the other sequentially using <font color=red>nn.Sequential</font>**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ39u6ksgm-7",
        "colab_type": "code",
        "outputId": "1fe9dd4a-83fb-4767-bd19-f3ae688c7000",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "input_size = 784\n",
        "hidden_sizes = [128, 64]\n",
        "output_size = 10\n",
        "\n",
        "# Build a feed-forward network\n",
        "modelSeq = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      nn.Softmax(dim=1))\n",
        "print(modelSeq)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (5): Softmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1LPY-bwTA1E",
        "colab_type": "text"
      },
      "source": [
        "We will be using option 1 to create Neural Network.\n",
        "\n",
        "**Weights and bias are automatically initialized for us for each layer and we can display them as:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOs3IdizNkKF",
        "colab_type": "code",
        "outputId": "686490c1-1423-45f7-b07a-97cf1d932042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        }
      },
      "source": [
        "print(model.fc1.weight)\n",
        "print(model.fc1.bias)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0178,  0.0171, -0.0183,  ...,  0.0231,  0.0054, -0.0070],\n",
            "        [-0.0011, -0.0337,  0.0011,  ..., -0.0090, -0.0355,  0.0188],\n",
            "        [-0.0245, -0.0275, -0.0013,  ..., -0.0212, -0.0161,  0.0109],\n",
            "        ...,\n",
            "        [ 0.0328, -0.0169,  0.0045,  ..., -0.0266, -0.0003,  0.0336],\n",
            "        [ 0.0083,  0.0225,  0.0048,  ..., -0.0275,  0.0137,  0.0056],\n",
            "        [-0.0283, -0.0211, -0.0157,  ..., -0.0309,  0.0211,  0.0107]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0167, -0.0230,  0.0240, -0.0220,  0.0018,  0.0253,  0.0301,  0.0098,\n",
            "         0.0242,  0.0093,  0.0153, -0.0219,  0.0294,  0.0172,  0.0213,  0.0111,\n",
            "         0.0243,  0.0148, -0.0250,  0.0169,  0.0219,  0.0354, -0.0031, -0.0350,\n",
            "         0.0218, -0.0332, -0.0230, -0.0059,  0.0149,  0.0337,  0.0233,  0.0274,\n",
            "         0.0234, -0.0007, -0.0155,  0.0185, -0.0327, -0.0114, -0.0042,  0.0321,\n",
            "        -0.0026,  0.0087, -0.0148, -0.0141,  0.0325,  0.0115, -0.0020, -0.0322,\n",
            "         0.0075, -0.0080,  0.0181,  0.0330,  0.0337, -0.0025, -0.0088, -0.0010,\n",
            "        -0.0342, -0.0200,  0.0343,  0.0051,  0.0085, -0.0241, -0.0170,  0.0084,\n",
            "        -0.0346,  0.0094, -0.0187, -0.0305, -0.0226,  0.0151,  0.0034,  0.0352,\n",
            "        -0.0339,  0.0013,  0.0316, -0.0291,  0.0334, -0.0207,  0.0262, -0.0341,\n",
            "        -0.0127,  0.0267,  0.0170, -0.0131,  0.0283, -0.0051,  0.0144,  0.0342,\n",
            "         0.0318, -0.0167,  0.0209,  0.0272, -0.0286, -0.0188, -0.0331,  0.0342,\n",
            "         0.0284,  0.0141, -0.0150,  0.0033, -0.0295,  0.0108, -0.0103, -0.0125,\n",
            "         0.0080, -0.0305, -0.0192, -0.0136, -0.0083,  0.0092, -0.0292,  0.0061,\n",
            "         0.0098,  0.0034, -0.0250, -0.0196, -0.0258, -0.0021, -0.0092,  0.0035,\n",
            "         0.0348,  0.0162, -0.0080,  0.0011,  0.0295,  0.0022,  0.0210, -0.0336],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuCFDThQmFRE",
        "colab_type": "text"
      },
      "source": [
        "### Let us see the output of our untrained Neural Network\n",
        "\n",
        "**Forward Pass:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTtL_e7iTYF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images , labels = next(iter(trainloader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTWYjrfwmtDk",
        "colab_type": "text"
      },
      "source": [
        "**Flattening the images to load in Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lrPJZIKmpe1",
        "colab_type": "code",
        "outputId": "12149e96-47bd-4244-85c4-da17efd05f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch_size = images.shape[0] #64 is batch size\n",
        "images.resize_(batch_size, 784)#in_place resizing of image\n",
        "print(images.size())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxHpn5VKvVdc",
        "colab_type": "text"
      },
      "source": [
        "**Checking the predictions of untrained network:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh6-wU_ivqHW",
        "colab_type": "code",
        "outputId": "75249572-63a2-47e0-cf67-3b846cf8f5a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "prob = model.forward(images)#classification probabilities based on untrained model\n",
        "print('Predicted Label:',np.argmax(prob[1].data.numpy()))\n",
        "print('Actual Label:', labels[1].numpy())#actual label"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Label: 7\n",
            "Actual Label: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jilDR8B3_DWK",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Training our Neural Network\n",
        "\n",
        "\n",
        "*   Define our neural network's loss function\n",
        "*   Define the backpropagation or optimization algorithm like SGD or ADAM\n",
        "*  To calculate gradient PyTorch uses a module called **[autograd](https://pytorch.org/docs/stable/autograd.html)**\n",
        "      * autograd keeps track of all the operations performed on a Tensor\n",
        "      * We need to set **requires_grad** on a tensor\n",
        "      *  We can do this ar creation with the requires_grad keyword, or at any time with **x.requires_grad_(True).**\n",
        "      * To compute the gradient, we need to call **backward()** method on the Tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dZUo7cBofbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lossCriteria = nn.CrossEntropyLoss() #defining loss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005) #lr is learning rate "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCVx4O26JfWR",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>***General process in PyTorch to update weights***<font>\n",
        "* Make a forward pass through the network to get the results/logits\n",
        "* Use the logits to calculate the loss\n",
        "* Perform a backward pass through the network with **loss.backward()** to calculate the gradients\n",
        "* Take a step with the optimizer to update the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNtzw_jyH8jU",
        "colab_type": "code",
        "outputId": "c4327ade-c7bc-4483-cc0a-e119d1fff674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "print('Weights Before Optimization ', model.fc1.weight)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights Before Optimization  Parameter containing:\n",
            "tensor([[ 0.0178,  0.0171, -0.0183,  ...,  0.0231,  0.0054, -0.0070],\n",
            "        [-0.0011, -0.0337,  0.0011,  ..., -0.0090, -0.0355,  0.0188],\n",
            "        [-0.0245, -0.0275, -0.0013,  ..., -0.0212, -0.0161,  0.0109],\n",
            "        ...,\n",
            "        [ 0.0328, -0.0169,  0.0045,  ..., -0.0266, -0.0003,  0.0336],\n",
            "        [ 0.0083,  0.0225,  0.0048,  ..., -0.0275,  0.0137,  0.0056],\n",
            "        [-0.0283, -0.0211, -0.0157,  ..., -0.0309,  0.0211,  0.0107]],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMDdAlJyLFkY",
        "colab_type": "code",
        "outputId": "f3857811-930b-4493-8dfe-7c1860fdf161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4135
        }
      },
      "source": [
        "epochs = 10\n",
        "print_every = 40 #to print the current loss after every 40 steps\n",
        "steps = 0\n",
        "for e in range(epochs):\n",
        "    running_loss = 0 #total loss till in previous 'print_every'steps\n",
        "    for images, labels in iter(trainloader):\n",
        "        steps += 1\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images.resize_(images.shape[0], 784)\n",
        "        \n",
        "        optimizer.zero_grad()#!!!VERY IMPORTANT. To ensure for each backpropagation, new gradients are computed\n",
        "        \n",
        "        # Forward and backward passes\n",
        "        output = model.forward(images)\n",
        "        loss = lossCriteria(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()#update weights\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if steps % print_every == 0:\n",
        "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
        "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
        "            \n",
        "            running_loss = 0"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10...  Loss: 2.3040\n",
            "Epoch: 1/10...  Loss: 2.3029\n",
            "Epoch: 1/10...  Loss: 2.3031\n",
            "Epoch: 1/10...  Loss: 2.3024\n",
            "Epoch: 1/10...  Loss: 2.3020\n",
            "Epoch: 1/10...  Loss: 2.3016\n",
            "Epoch: 1/10...  Loss: 2.3012\n",
            "Epoch: 1/10...  Loss: 2.3013\n",
            "Epoch: 1/10...  Loss: 2.3012\n",
            "Epoch: 1/10...  Loss: 2.3006\n",
            "Epoch: 1/10...  Loss: 2.3000\n",
            "Epoch: 1/10...  Loss: 2.2996\n",
            "Epoch: 1/10...  Loss: 2.2990\n",
            "Epoch: 1/10...  Loss: 2.2989\n",
            "Epoch: 1/10...  Loss: 2.2990\n",
            "Epoch: 1/10...  Loss: 2.2983\n",
            "Epoch: 1/10...  Loss: 2.2984\n",
            "Epoch: 1/10...  Loss: 2.2981\n",
            "Epoch: 1/10...  Loss: 2.2973\n",
            "Epoch: 1/10...  Loss: 2.2971\n",
            "Epoch: 1/10...  Loss: 2.2965\n",
            "Epoch: 1/10...  Loss: 2.2963\n",
            "Epoch: 1/10...  Loss: 2.2960\n",
            "Epoch: 2/10...  Loss: 1.2625\n",
            "Epoch: 2/10...  Loss: 2.2953\n",
            "Epoch: 2/10...  Loss: 2.2948\n",
            "Epoch: 2/10...  Loss: 2.2941\n",
            "Epoch: 2/10...  Loss: 2.2945\n",
            "Epoch: 2/10...  Loss: 2.2922\n",
            "Epoch: 2/10...  Loss: 2.2924\n",
            "Epoch: 2/10...  Loss: 2.2924\n",
            "Epoch: 2/10...  Loss: 2.2912\n",
            "Epoch: 2/10...  Loss: 2.2910\n",
            "Epoch: 2/10...  Loss: 2.2900\n",
            "Epoch: 2/10...  Loss: 2.2895\n",
            "Epoch: 2/10...  Loss: 2.2893\n",
            "Epoch: 2/10...  Loss: 2.2892\n",
            "Epoch: 2/10...  Loss: 2.2879\n",
            "Epoch: 2/10...  Loss: 2.2864\n",
            "Epoch: 2/10...  Loss: 2.2853\n",
            "Epoch: 2/10...  Loss: 2.2836\n",
            "Epoch: 2/10...  Loss: 2.2836\n",
            "Epoch: 2/10...  Loss: 2.2835\n",
            "Epoch: 2/10...  Loss: 2.2805\n",
            "Epoch: 2/10...  Loss: 2.2801\n",
            "Epoch: 2/10...  Loss: 2.2780\n",
            "Epoch: 3/10...  Loss: 0.2284\n",
            "Epoch: 3/10...  Loss: 2.2749\n",
            "Epoch: 3/10...  Loss: 2.2755\n",
            "Epoch: 3/10...  Loss: 2.2730\n",
            "Epoch: 3/10...  Loss: 2.2701\n",
            "Epoch: 3/10...  Loss: 2.2683\n",
            "Epoch: 3/10...  Loss: 2.2681\n",
            "Epoch: 3/10...  Loss: 2.2651\n",
            "Epoch: 3/10...  Loss: 2.2623\n",
            "Epoch: 3/10...  Loss: 2.2572\n",
            "Epoch: 3/10...  Loss: 2.2585\n",
            "Epoch: 3/10...  Loss: 2.2561\n",
            "Epoch: 3/10...  Loss: 2.2467\n",
            "Epoch: 3/10...  Loss: 2.2461\n",
            "Epoch: 3/10...  Loss: 2.2508\n",
            "Epoch: 3/10...  Loss: 2.2405\n",
            "Epoch: 3/10...  Loss: 2.2391\n",
            "Epoch: 3/10...  Loss: 2.2316\n",
            "Epoch: 3/10...  Loss: 2.2301\n",
            "Epoch: 3/10...  Loss: 2.2296\n",
            "Epoch: 3/10...  Loss: 2.2254\n",
            "Epoch: 3/10...  Loss: 2.2117\n",
            "Epoch: 3/10...  Loss: 2.2109\n",
            "Epoch: 3/10...  Loss: 2.2045\n",
            "Epoch: 4/10...  Loss: 1.4374\n",
            "Epoch: 4/10...  Loss: 2.1937\n",
            "Epoch: 4/10...  Loss: 2.1818\n",
            "Epoch: 4/10...  Loss: 2.1773\n",
            "Epoch: 4/10...  Loss: 2.1659\n",
            "Epoch: 4/10...  Loss: 2.1648\n",
            "Epoch: 4/10...  Loss: 2.1637\n",
            "Epoch: 4/10...  Loss: 2.1525\n",
            "Epoch: 4/10...  Loss: 2.1442\n",
            "Epoch: 4/10...  Loss: 2.1367\n",
            "Epoch: 4/10...  Loss: 2.1320\n",
            "Epoch: 4/10...  Loss: 2.1253\n",
            "Epoch: 4/10...  Loss: 2.1188\n",
            "Epoch: 4/10...  Loss: 2.1126\n",
            "Epoch: 4/10...  Loss: 2.1048\n",
            "Epoch: 4/10...  Loss: 2.1218\n",
            "Epoch: 4/10...  Loss: 2.1053\n",
            "Epoch: 4/10...  Loss: 2.0992\n",
            "Epoch: 4/10...  Loss: 2.0922\n",
            "Epoch: 4/10...  Loss: 2.0999\n",
            "Epoch: 4/10...  Loss: 2.0879\n",
            "Epoch: 4/10...  Loss: 2.0875\n",
            "Epoch: 4/10...  Loss: 2.0840\n",
            "Epoch: 5/10...  Loss: 0.4138\n",
            "Epoch: 5/10...  Loss: 2.0620\n",
            "Epoch: 5/10...  Loss: 2.0723\n",
            "Epoch: 5/10...  Loss: 2.0552\n",
            "Epoch: 5/10...  Loss: 2.0583\n",
            "Epoch: 5/10...  Loss: 2.0467\n",
            "Epoch: 5/10...  Loss: 2.0559\n",
            "Epoch: 5/10...  Loss: 2.0624\n",
            "Epoch: 5/10...  Loss: 2.0451\n",
            "Epoch: 5/10...  Loss: 2.0358\n",
            "Epoch: 5/10...  Loss: 2.0298\n",
            "Epoch: 5/10...  Loss: 2.0291\n",
            "Epoch: 5/10...  Loss: 2.0316\n",
            "Epoch: 5/10...  Loss: 2.0206\n",
            "Epoch: 5/10...  Loss: 2.0193\n",
            "Epoch: 5/10...  Loss: 2.0078\n",
            "Epoch: 5/10...  Loss: 2.0062\n",
            "Epoch: 5/10...  Loss: 2.0127\n",
            "Epoch: 5/10...  Loss: 2.0150\n",
            "Epoch: 5/10...  Loss: 1.9899\n",
            "Epoch: 5/10...  Loss: 2.0020\n",
            "Epoch: 5/10...  Loss: 1.9803\n",
            "Epoch: 5/10...  Loss: 1.9897\n",
            "Epoch: 5/10...  Loss: 1.9772\n",
            "Epoch: 6/10...  Loss: 1.4817\n",
            "Epoch: 6/10...  Loss: 1.9851\n",
            "Epoch: 6/10...  Loss: 1.9624\n",
            "Epoch: 6/10...  Loss: 1.9707\n",
            "Epoch: 6/10...  Loss: 1.9514\n",
            "Epoch: 6/10...  Loss: 1.9462\n",
            "Epoch: 6/10...  Loss: 1.9406\n",
            "Epoch: 6/10...  Loss: 1.9431\n",
            "Epoch: 6/10...  Loss: 1.9333\n",
            "Epoch: 6/10...  Loss: 1.9389\n",
            "Epoch: 6/10...  Loss: 1.9280\n",
            "Epoch: 6/10...  Loss: 1.9212\n",
            "Epoch: 6/10...  Loss: 1.9210\n",
            "Epoch: 6/10...  Loss: 1.9263\n",
            "Epoch: 6/10...  Loss: 1.9095\n",
            "Epoch: 6/10...  Loss: 1.8965\n",
            "Epoch: 6/10...  Loss: 1.9156\n",
            "Epoch: 6/10...  Loss: 1.8883\n",
            "Epoch: 6/10...  Loss: 1.8937\n",
            "Epoch: 6/10...  Loss: 1.8894\n",
            "Epoch: 6/10...  Loss: 1.8864\n",
            "Epoch: 6/10...  Loss: 1.8932\n",
            "Epoch: 6/10...  Loss: 1.8634\n",
            "Epoch: 7/10...  Loss: 0.5635\n",
            "Epoch: 7/10...  Loss: 1.8847\n",
            "Epoch: 7/10...  Loss: 1.8654\n",
            "Epoch: 7/10...  Loss: 1.8563\n",
            "Epoch: 7/10...  Loss: 1.8538\n",
            "Epoch: 7/10...  Loss: 1.8442\n",
            "Epoch: 7/10...  Loss: 1.8496\n",
            "Epoch: 7/10...  Loss: 1.8430\n",
            "Epoch: 7/10...  Loss: 1.8485\n",
            "Epoch: 7/10...  Loss: 1.8282\n",
            "Epoch: 7/10...  Loss: 1.8523\n",
            "Epoch: 7/10...  Loss: 1.8275\n",
            "Epoch: 7/10...  Loss: 1.8297\n",
            "Epoch: 7/10...  Loss: 1.8361\n",
            "Epoch: 7/10...  Loss: 1.8253\n",
            "Epoch: 7/10...  Loss: 1.8149\n",
            "Epoch: 7/10...  Loss: 1.8104\n",
            "Epoch: 7/10...  Loss: 1.8138\n",
            "Epoch: 7/10...  Loss: 1.8237\n",
            "Epoch: 7/10...  Loss: 1.8137\n",
            "Epoch: 7/10...  Loss: 1.8081\n",
            "Epoch: 7/10...  Loss: 1.8103\n",
            "Epoch: 7/10...  Loss: 1.8109\n",
            "Epoch: 7/10...  Loss: 1.7953\n",
            "Epoch: 8/10...  Loss: 1.5361\n",
            "Epoch: 8/10...  Loss: 1.7826\n",
            "Epoch: 8/10...  Loss: 1.7950\n",
            "Epoch: 8/10...  Loss: 1.7951\n",
            "Epoch: 8/10...  Loss: 1.7968\n",
            "Epoch: 8/10...  Loss: 1.8007\n",
            "Epoch: 8/10...  Loss: 1.7950\n",
            "Epoch: 8/10...  Loss: 1.7818\n",
            "Epoch: 8/10...  Loss: 1.7862\n",
            "Epoch: 8/10...  Loss: 1.7878\n",
            "Epoch: 8/10...  Loss: 1.7843\n",
            "Epoch: 8/10...  Loss: 1.7817\n",
            "Epoch: 8/10...  Loss: 1.7898\n",
            "Epoch: 8/10...  Loss: 1.7745\n",
            "Epoch: 8/10...  Loss: 1.7812\n",
            "Epoch: 8/10...  Loss: 1.7878\n",
            "Epoch: 8/10...  Loss: 1.7846\n",
            "Epoch: 8/10...  Loss: 1.7785\n",
            "Epoch: 8/10...  Loss: 1.7682\n",
            "Epoch: 8/10...  Loss: 1.7842\n",
            "Epoch: 8/10...  Loss: 1.7843\n",
            "Epoch: 8/10...  Loss: 1.7588\n",
            "Epoch: 8/10...  Loss: 1.7730\n",
            "Epoch: 9/10...  Loss: 0.7002\n",
            "Epoch: 9/10...  Loss: 1.7754\n",
            "Epoch: 9/10...  Loss: 1.7645\n",
            "Epoch: 9/10...  Loss: 1.7785\n",
            "Epoch: 9/10...  Loss: 1.7574\n",
            "Epoch: 9/10...  Loss: 1.7696\n",
            "Epoch: 9/10...  Loss: 1.7624\n",
            "Epoch: 9/10...  Loss: 1.7910\n",
            "Epoch: 9/10...  Loss: 1.7568\n",
            "Epoch: 9/10...  Loss: 1.7635\n",
            "Epoch: 9/10...  Loss: 1.7691\n",
            "Epoch: 9/10...  Loss: 1.7589\n",
            "Epoch: 9/10...  Loss: 1.7649\n",
            "Epoch: 9/10...  Loss: 1.7612\n",
            "Epoch: 9/10...  Loss: 1.7663\n",
            "Epoch: 9/10...  Loss: 1.7580\n",
            "Epoch: 9/10...  Loss: 1.7581\n",
            "Epoch: 9/10...  Loss: 1.7442\n",
            "Epoch: 9/10...  Loss: 1.7640\n",
            "Epoch: 9/10...  Loss: 1.7498\n",
            "Epoch: 9/10...  Loss: 1.7594\n",
            "Epoch: 9/10...  Loss: 1.7537\n",
            "Epoch: 9/10...  Loss: 1.7579\n",
            "Epoch: 9/10...  Loss: 1.7520\n",
            "Epoch: 10/10...  Loss: 1.6653\n",
            "Epoch: 10/10...  Loss: 1.7532\n",
            "Epoch: 10/10...  Loss: 1.7538\n",
            "Epoch: 10/10...  Loss: 1.7385\n",
            "Epoch: 10/10...  Loss: 1.7492\n",
            "Epoch: 10/10...  Loss: 1.7552\n",
            "Epoch: 10/10...  Loss: 1.7322\n",
            "Epoch: 10/10...  Loss: 1.7583\n",
            "Epoch: 10/10...  Loss: 1.7444\n",
            "Epoch: 10/10...  Loss: 1.7528\n",
            "Epoch: 10/10...  Loss: 1.7546\n",
            "Epoch: 10/10...  Loss: 1.7620\n",
            "Epoch: 10/10...  Loss: 1.7412\n",
            "Epoch: 10/10...  Loss: 1.7532\n",
            "Epoch: 10/10...  Loss: 1.7552\n",
            "Epoch: 10/10...  Loss: 1.7481\n",
            "Epoch: 10/10...  Loss: 1.7420\n",
            "Epoch: 10/10...  Loss: 1.7482\n",
            "Epoch: 10/10...  Loss: 1.7433\n",
            "Epoch: 10/10...  Loss: 1.7541\n",
            "Epoch: 10/10...  Loss: 1.7486\n",
            "Epoch: 10/10...  Loss: 1.7426\n",
            "Epoch: 10/10...  Loss: 1.7485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfX0ofD4W2Pn",
        "colab_type": "code",
        "outputId": "95f1b9b3-893b-4895-953e-869c52716114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "print('Weights After Optimization ', model.fc1.weight)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights After Optimization  Parameter containing:\n",
            "tensor([[ 1.8585e-02,  1.7904e-02, -1.7466e-02,  ...,  2.3898e-02,\n",
            "          6.2186e-03, -6.2227e-03],\n",
            "        [-4.2546e-03, -3.6891e-02, -2.1334e-03,  ..., -1.2147e-02,\n",
            "         -3.8694e-02,  1.5573e-02],\n",
            "        [-2.8616e-02, -3.1597e-02, -5.3587e-03,  ..., -2.5263e-02,\n",
            "         -2.0233e-02,  6.7590e-03],\n",
            "        ...,\n",
            "        [ 3.3017e-02, -1.6635e-02,  4.7131e-03,  ..., -2.6397e-02,\n",
            "         -1.6155e-05,  3.3824e-02],\n",
            "        [ 3.2108e-03,  1.7413e-02, -2.1501e-04,  ..., -3.2502e-02,\n",
            "          8.6546e-03,  5.0302e-04],\n",
            "        [-2.8986e-02, -2.1787e-02, -1.6343e-02,  ..., -3.1566e-02,\n",
            "          2.0374e-02,  9.9717e-03]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xa9f9WQJOwz",
        "colab_type": "text"
      },
      "source": [
        "**Checking the predictions of trained network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJmC0QKlJebv",
        "colab_type": "code",
        "outputId": "fdb116b0-0e73-4d55-fc66-3439edace3f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "model.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(testloader))\n",
        "batch_size = images.shape[0] #64 is batch size\n",
        "images.resize_(batch_size, 784)#in_place resizing of image\n",
        "print(images.size())\n",
        "output = model.forward(images)#classification probabilities based on untrained model\n",
        "print('Predicted Label:',np.argmax(output[1].data.numpy()))\n",
        "print('Actual Label:', labels[1].numpy())#actual label"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 784])\n",
            "Predicted Label: 8\n",
            "Actual Label: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPXPQSZqmPYL",
        "colab_type": "text"
      },
      "source": [
        "## Challenge 2.1  \n",
        "Build a network to classify the MNIST images with three hidden layers **using option 2** as discussed above. Use 400 units in the first hidden layer, 200 units in the second layer, and 100 units in the third layer. Each hidden layer should have a ReLU activation function, and use softmax on the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awkJ8VrCPfG4",
        "colab_type": "code",
        "outputId": "704b0834-6554-4acf-b912-19be82a27bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        }
      },
      "source": [
        "#Make changes in this cell\n",
        "#dataset aleady loaded\n",
        "#all required modules are already imported\n",
        "input_size = 784\n",
        "hidden_sizes = ['Fix_Me','Fix_Me',\"Fix_Me\"]\n",
        "output_size = 10\n",
        "\n",
        "# Build a feed-forward network\n",
        "#Fix Me - Add layers here. Take help from above cells\n",
        "model3 = nn.Sequential(nn.Linear(input_size, 'Fix_Me'),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear('Fix_Me', 'Fix_Me'),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear('Fix_Me', 'Fix_Me'),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear('Fix_Me',output_size),\n",
        "                      nn.Softmax(dim=1))\n",
        "print(model3)\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-8b9b328bf849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Build a feed-forward network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Fix Me - Add layers here. Take help from above cells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m model3 = nn.Sequential(nn.Linear(input_size, 'Fix_Me'),\n\u001b[0m\u001b[1;32m      8\u001b[0m                       \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                       \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fix_Me'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Fix_Me'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (str, int), but expected one of:\n * (torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, torch.device device)\n      didn't match because some of the arguments have invalid types: (!str!, !int!)\n * (object data, torch.device device)\n      didn't match because some of the arguments have invalid types: (!str!, !int!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPEesvrqJNjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Do not change anything in this cell\n",
        "lossCriteria = nn.CrossEntropyLoss() #defining loss\n",
        "optimizer = torch.optim.SGD(model3.parameters(), lr=0.005) #lr is learning rate \n",
        "epochs = 10\n",
        "print_every = 40 #to print the current loss after every 40 steps\n",
        "steps = 0\n",
        "for e in range(epochs):\n",
        "    running_loss = 0 #total loss till in previous 'print_every'steps\n",
        "    for images, labels in iter(trainloader):\n",
        "        steps += 1\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images.resize_(images.shape[0], 784)\n",
        "        \n",
        "        optimizer.zero_grad()#!!!VERY IMPORTANT. To ensure for each backpropagation, new gradients are computed\n",
        "        \n",
        "        # Forward and backward passes\n",
        "        output = model3.forward(images)\n",
        "        loss = lossCriteria(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()#update weights\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if steps % print_every == 0:\n",
        "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
        "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
        "            \n",
        "            running_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ-6tDEVVD7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Do not change anything in this cell\n",
        "#Testing predictions of trained model\n",
        "model3.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(testloader))\n",
        "batch_size = images.shape[0] #64 is batch size\n",
        "images.resize_(batch_size, 784)#in_place resizing of image\n",
        "print(images.size())\n",
        "prob = model3.forward(images)#classification probabilities based on untrained model\n",
        "print('Predicted Label:',np.argmax(prob[1].data.numpy()))\n",
        "print('Actual Label:', labels[1].numpy())#actual label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChZoq9dPSI9l",
        "colab_type": "text"
      },
      "source": [
        "## Challenge 2.2 \n",
        "\n",
        "Design a Deep Neural Network (and name it model4) for classification of images based on **[Fashion-MNIST](https://pytorch.org/docs/stable/torchvision/datasets.html#fashion-mnist)** dataset. Here the images are of dimension 28 * 28 and we have to classify them in one out of 10 categories.\n",
        "\n",
        "![alt text](https://github.com/zalandoresearch/fashion-mnist/blob/master/doc/img/fashion-mnist-sprite.png?raw=true)\n",
        "\n",
        "For more details **[click here](https://github.com/zalandoresearch/fashion-mnist)**. Your challenge is to:\n",
        "\n",
        "\n",
        "*   Load the dataset\n",
        "*   Build the network\n",
        "*  Train the network\n",
        "* Try to predict some random samples\n",
        "\n",
        "You can take the help of above code and make necessary changes to solve this challenge.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvwdgT_VmS3I",
        "colab_type": "code",
        "outputId": "8db8cb73-305a-4802-d8ce-a8a39311fc17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "#load dataset in this cell"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to FashionMNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:01, 13628959.20it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting FashionMNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to FashionMNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 97076.76it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting FashionMNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to FashionMNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:01, 4069187.66it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting FashionMNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to FashionMNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 29542.89it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting FashionMNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9BNGOzlUV9H",
        "colab_type": "code",
        "outputId": "888217d6-40b5-4664-ffc7-aa5e74015b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "# Build a feed-forward network"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=400, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=400, out_features=200, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=200, out_features=100, bias=True)\n",
            "  (5): ReLU()\n",
            "  (6): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (7): Softmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFAjBs40wcDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Specify loss and optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8J4BuAGUYuR",
        "colab_type": "code",
        "outputId": "c6be3df9-a60c-428a-f515-9fc7bbcca7c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4156
        }
      },
      "source": [
        "#train network in this cell"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10...  Loss: 2.3025\n",
            "Epoch: 1/10...  Loss: 2.3025\n",
            "Epoch: 1/10...  Loss: 2.3020\n",
            "Epoch: 1/10...  Loss: 2.3020\n",
            "Epoch: 1/10...  Loss: 2.3020\n",
            "Epoch: 1/10...  Loss: 2.3017\n",
            "Epoch: 1/10...  Loss: 2.3015\n",
            "Epoch: 1/10...  Loss: 2.3016\n",
            "Epoch: 1/10...  Loss: 2.3012\n",
            "Epoch: 1/10...  Loss: 2.3014\n",
            "Epoch: 1/10...  Loss: 2.3010\n",
            "Epoch: 1/10...  Loss: 2.3009\n",
            "Epoch: 1/10...  Loss: 2.3006\n",
            "Epoch: 1/10...  Loss: 2.3003\n",
            "Epoch: 1/10...  Loss: 2.3005\n",
            "Epoch: 1/10...  Loss: 2.3000\n",
            "Epoch: 1/10...  Loss: 2.3000\n",
            "Epoch: 1/10...  Loss: 2.2998\n",
            "Epoch: 1/10...  Loss: 2.2997\n",
            "Epoch: 1/10...  Loss: 2.2997\n",
            "Epoch: 1/10...  Loss: 2.2995\n",
            "Epoch: 1/10...  Loss: 2.2992\n",
            "Epoch: 1/10...  Loss: 2.2992\n",
            "Epoch: 2/10...  Loss: 1.2644\n",
            "Epoch: 2/10...  Loss: 2.2986\n",
            "Epoch: 2/10...  Loss: 2.2990\n",
            "Epoch: 2/10...  Loss: 2.2983\n",
            "Epoch: 2/10...  Loss: 2.2981\n",
            "Epoch: 2/10...  Loss: 2.2979\n",
            "Epoch: 2/10...  Loss: 2.2979\n",
            "Epoch: 2/10...  Loss: 2.2972\n",
            "Epoch: 2/10...  Loss: 2.2975\n",
            "Epoch: 2/10...  Loss: 2.2969\n",
            "Epoch: 2/10...  Loss: 2.2970\n",
            "Epoch: 2/10...  Loss: 2.2963\n",
            "Epoch: 2/10...  Loss: 2.2965\n",
            "Epoch: 2/10...  Loss: 2.2963\n",
            "Epoch: 2/10...  Loss: 2.2960\n",
            "Epoch: 2/10...  Loss: 2.2959\n",
            "Epoch: 2/10...  Loss: 2.2958\n",
            "Epoch: 2/10...  Loss: 2.2952\n",
            "Epoch: 2/10...  Loss: 2.2949\n",
            "Epoch: 2/10...  Loss: 2.2950\n",
            "Epoch: 2/10...  Loss: 2.2945\n",
            "Epoch: 2/10...  Loss: 2.2937\n",
            "Epoch: 2/10...  Loss: 2.2935\n",
            "Epoch: 3/10...  Loss: 0.2294\n",
            "Epoch: 3/10...  Loss: 2.2930\n",
            "Epoch: 3/10...  Loss: 2.2927\n",
            "Epoch: 3/10...  Loss: 2.2922\n",
            "Epoch: 3/10...  Loss: 2.2918\n",
            "Epoch: 3/10...  Loss: 2.2914\n",
            "Epoch: 3/10...  Loss: 2.2913\n",
            "Epoch: 3/10...  Loss: 2.2906\n",
            "Epoch: 3/10...  Loss: 2.2899\n",
            "Epoch: 3/10...  Loss: 2.2900\n",
            "Epoch: 3/10...  Loss: 2.2892\n",
            "Epoch: 3/10...  Loss: 2.2883\n",
            "Epoch: 3/10...  Loss: 2.2884\n",
            "Epoch: 3/10...  Loss: 2.2872\n",
            "Epoch: 3/10...  Loss: 2.2867\n",
            "Epoch: 3/10...  Loss: 2.2858\n",
            "Epoch: 3/10...  Loss: 2.2852\n",
            "Epoch: 3/10...  Loss: 2.2835\n",
            "Epoch: 3/10...  Loss: 2.2839\n",
            "Epoch: 3/10...  Loss: 2.2823\n",
            "Epoch: 3/10...  Loss: 2.2813\n",
            "Epoch: 3/10...  Loss: 2.2817\n",
            "Epoch: 3/10...  Loss: 2.2807\n",
            "Epoch: 3/10...  Loss: 2.2778\n",
            "Epoch: 4/10...  Loss: 1.4797\n",
            "Epoch: 4/10...  Loss: 2.2759\n",
            "Epoch: 4/10...  Loss: 2.2735\n",
            "Epoch: 4/10...  Loss: 2.2728\n",
            "Epoch: 4/10...  Loss: 2.2698\n",
            "Epoch: 4/10...  Loss: 2.2681\n",
            "Epoch: 4/10...  Loss: 2.2662\n",
            "Epoch: 4/10...  Loss: 2.2611\n",
            "Epoch: 4/10...  Loss: 2.2586\n",
            "Epoch: 4/10...  Loss: 2.2622\n",
            "Epoch: 4/10...  Loss: 2.2560\n",
            "Epoch: 4/10...  Loss: 2.2557\n",
            "Epoch: 4/10...  Loss: 2.2489\n",
            "Epoch: 4/10...  Loss: 2.2443\n",
            "Epoch: 4/10...  Loss: 2.2419\n",
            "Epoch: 4/10...  Loss: 2.2332\n",
            "Epoch: 4/10...  Loss: 2.2299\n",
            "Epoch: 4/10...  Loss: 2.2330\n",
            "Epoch: 4/10...  Loss: 2.2189\n",
            "Epoch: 4/10...  Loss: 2.2192\n",
            "Epoch: 4/10...  Loss: 2.2157\n",
            "Epoch: 4/10...  Loss: 2.2116\n",
            "Epoch: 4/10...  Loss: 2.2103\n",
            "Epoch: 5/10...  Loss: 0.4426\n",
            "Epoch: 5/10...  Loss: 2.1882\n",
            "Epoch: 5/10...  Loss: 2.1774\n",
            "Epoch: 5/10...  Loss: 2.1819\n",
            "Epoch: 5/10...  Loss: 2.1819\n",
            "Epoch: 5/10...  Loss: 2.1820\n",
            "Epoch: 5/10...  Loss: 2.1759\n",
            "Epoch: 5/10...  Loss: 2.1723\n",
            "Epoch: 5/10...  Loss: 2.1718\n",
            "Epoch: 5/10...  Loss: 2.1693\n",
            "Epoch: 5/10...  Loss: 2.1729\n",
            "Epoch: 5/10...  Loss: 2.1585\n",
            "Epoch: 5/10...  Loss: 2.1565\n",
            "Epoch: 5/10...  Loss: 2.1660\n",
            "Epoch: 5/10...  Loss: 2.1474\n",
            "Epoch: 5/10...  Loss: 2.1451\n",
            "Epoch: 5/10...  Loss: 2.1524\n",
            "Epoch: 5/10...  Loss: 2.1417\n",
            "Epoch: 5/10...  Loss: 2.1337\n",
            "Epoch: 5/10...  Loss: 2.1420\n",
            "Epoch: 5/10...  Loss: 2.1356\n",
            "Epoch: 5/10...  Loss: 2.1276\n",
            "Epoch: 5/10...  Loss: 2.1311\n",
            "Epoch: 5/10...  Loss: 2.1209\n",
            "Epoch: 6/10...  Loss: 1.5833\n",
            "Epoch: 6/10...  Loss: 2.1086\n",
            "Epoch: 6/10...  Loss: 2.1020\n",
            "Epoch: 6/10...  Loss: 2.1027\n",
            "Epoch: 6/10...  Loss: 2.1043\n",
            "Epoch: 6/10...  Loss: 2.0912\n",
            "Epoch: 6/10...  Loss: 2.0775\n",
            "Epoch: 6/10...  Loss: 2.0676\n",
            "Epoch: 6/10...  Loss: 2.0699\n",
            "Epoch: 6/10...  Loss: 2.0696\n",
            "Epoch: 6/10...  Loss: 2.0448\n",
            "Epoch: 6/10...  Loss: 2.0436\n",
            "Epoch: 6/10...  Loss: 2.0284\n",
            "Epoch: 6/10...  Loss: 2.0238\n",
            "Epoch: 6/10...  Loss: 2.0139\n",
            "Epoch: 6/10...  Loss: 2.0062\n",
            "Epoch: 6/10...  Loss: 1.9960\n",
            "Epoch: 6/10...  Loss: 2.0032\n",
            "Epoch: 6/10...  Loss: 1.9967\n",
            "Epoch: 6/10...  Loss: 1.9918\n",
            "Epoch: 6/10...  Loss: 1.9860\n",
            "Epoch: 6/10...  Loss: 1.9788\n",
            "Epoch: 6/10...  Loss: 1.9778\n",
            "Epoch: 7/10...  Loss: 0.5864\n",
            "Epoch: 7/10...  Loss: 1.9724\n",
            "Epoch: 7/10...  Loss: 1.9580\n",
            "Epoch: 7/10...  Loss: 1.9569\n",
            "Epoch: 7/10...  Loss: 1.9477\n",
            "Epoch: 7/10...  Loss: 1.9451\n",
            "Epoch: 7/10...  Loss: 1.9405\n",
            "Epoch: 7/10...  Loss: 1.9370\n",
            "Epoch: 7/10...  Loss: 1.9414\n",
            "Epoch: 7/10...  Loss: 1.9296\n",
            "Epoch: 7/10...  Loss: 1.9141\n",
            "Epoch: 7/10...  Loss: 1.9250\n",
            "Epoch: 7/10...  Loss: 1.9128\n",
            "Epoch: 7/10...  Loss: 1.9077\n",
            "Epoch: 7/10...  Loss: 1.9250\n",
            "Epoch: 7/10...  Loss: 1.9059\n",
            "Epoch: 7/10...  Loss: 1.9043\n",
            "Epoch: 7/10...  Loss: 1.8990\n",
            "Epoch: 7/10...  Loss: 1.9072\n",
            "Epoch: 7/10...  Loss: 1.8960\n",
            "Epoch: 7/10...  Loss: 1.8772\n",
            "Epoch: 7/10...  Loss: 1.8865\n",
            "Epoch: 7/10...  Loss: 1.8798\n",
            "Epoch: 7/10...  Loss: 1.8835\n",
            "Epoch: 8/10...  Loss: 1.5961\n",
            "Epoch: 8/10...  Loss: 1.8674\n",
            "Epoch: 8/10...  Loss: 1.8764\n",
            "Epoch: 8/10...  Loss: 1.8681\n",
            "Epoch: 8/10...  Loss: 1.8647\n",
            "Epoch: 8/10...  Loss: 1.8637\n",
            "Epoch: 8/10...  Loss: 1.8478\n",
            "Epoch: 8/10...  Loss: 1.8344\n",
            "Epoch: 8/10...  Loss: 1.8604\n",
            "Epoch: 8/10...  Loss: 1.8560\n",
            "Epoch: 8/10...  Loss: 1.8439\n",
            "Epoch: 8/10...  Loss: 1.8390\n",
            "Epoch: 8/10...  Loss: 1.8308\n",
            "Epoch: 8/10...  Loss: 1.8262\n",
            "Epoch: 8/10...  Loss: 1.8237\n",
            "Epoch: 8/10...  Loss: 1.8260\n",
            "Epoch: 8/10...  Loss: 1.8175\n",
            "Epoch: 8/10...  Loss: 1.8143\n",
            "Epoch: 8/10...  Loss: 1.8013\n",
            "Epoch: 8/10...  Loss: 1.8089\n",
            "Epoch: 8/10...  Loss: 1.8178\n",
            "Epoch: 8/10...  Loss: 1.8045\n",
            "Epoch: 8/10...  Loss: 1.7972\n",
            "Epoch: 9/10...  Loss: 0.7164\n",
            "Epoch: 9/10...  Loss: 1.7925\n",
            "Epoch: 9/10...  Loss: 1.7971\n",
            "Epoch: 9/10...  Loss: 1.7849\n",
            "Epoch: 9/10...  Loss: 1.7699\n",
            "Epoch: 9/10...  Loss: 1.7899\n",
            "Epoch: 9/10...  Loss: 1.7782\n",
            "Epoch: 9/10...  Loss: 1.7911\n",
            "Epoch: 9/10...  Loss: 1.7849\n",
            "Epoch: 9/10...  Loss: 1.7782\n",
            "Epoch: 9/10...  Loss: 1.7763\n",
            "Epoch: 9/10...  Loss: 1.7739\n",
            "Epoch: 9/10...  Loss: 1.7722\n",
            "Epoch: 9/10...  Loss: 1.7675\n",
            "Epoch: 9/10...  Loss: 1.7626\n",
            "Epoch: 9/10...  Loss: 1.7602\n",
            "Epoch: 9/10...  Loss: 1.7765\n",
            "Epoch: 9/10...  Loss: 1.7589\n",
            "Epoch: 9/10...  Loss: 1.7621\n",
            "Epoch: 9/10...  Loss: 1.7716\n",
            "Epoch: 9/10...  Loss: 1.7612\n",
            "Epoch: 9/10...  Loss: 1.7630\n",
            "Epoch: 9/10...  Loss: 1.7579\n",
            "Epoch: 9/10...  Loss: 1.7541\n",
            "Epoch: 10/10...  Loss: 1.6601\n",
            "Epoch: 10/10...  Loss: 1.7525\n",
            "Epoch: 10/10...  Loss: 1.7366\n",
            "Epoch: 10/10...  Loss: 1.7591\n",
            "Epoch: 10/10...  Loss: 1.7640\n",
            "Epoch: 10/10...  Loss: 1.7499\n",
            "Epoch: 10/10...  Loss: 1.7498\n",
            "Epoch: 10/10...  Loss: 1.7611\n",
            "Epoch: 10/10...  Loss: 1.7600\n",
            "Epoch: 10/10...  Loss: 1.7456\n",
            "Epoch: 10/10...  Loss: 1.7490\n",
            "Epoch: 10/10...  Loss: 1.7465\n",
            "Epoch: 10/10...  Loss: 1.7415\n",
            "Epoch: 10/10...  Loss: 1.7467\n",
            "Epoch: 10/10...  Loss: 1.7466\n",
            "Epoch: 10/10...  Loss: 1.7340\n",
            "Epoch: 10/10...  Loss: 1.7343\n",
            "Epoch: 10/10...  Loss: 1.7477\n",
            "Epoch: 10/10...  Loss: 1.7377\n",
            "Epoch: 10/10...  Loss: 1.7314\n",
            "Epoch: 10/10...  Loss: 1.7279\n",
            "Epoch: 10/10...  Loss: 1.7279\n",
            "Epoch: 10/10...  Loss: 1.7415\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzEFat9ZUc8P",
        "colab_type": "code",
        "outputId": "2bd85e3e-f707-4b91-bc34-ff59f67c55fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "#make prediction(s) in this cell"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 784])\n",
            "Predicted Label: 5\n",
            "Actual Label: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATGst6rR2Fgq",
        "colab_type": "text"
      },
      "source": [
        "# 3. Computer Vision using CNN in PyTorch\n",
        "\n",
        "\n",
        "*   We will be creating a simple CNN for classifying images in **[CIFAR10 dataset](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar)**\n",
        "\n",
        "![CIFAR10](https://storage.googleapis.com/kaggle-competitions/kaggle/3649/media/cifar-10.png)\n",
        "\n",
        "*   The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class\n",
        "* There are 50000 training images and 10000 test images. \n",
        "\n",
        "\n",
        "**Loading and transforming the dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0mFMdFF6riZ",
        "colab_type": "code",
        "outputId": "6f17fe06-0681-4eba-de3c-467945d3ffbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = datasets.CIFAR10('CIFAR10/', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = datasets.CIFAR10(root='CIFAR10/', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=True, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQH9YTZJ9AlU",
        "colab_type": "text"
      },
      "source": [
        "**Creating CNN based Deep Neural Network having following layout**\n",
        "\n",
        "\n",
        "*   Input > Conv (ReLU) > MaxPool > Conv (ReLU) > MaxPool > FC (ReLU) > FC (ReLU) > FC (SoftMax) > 10 outputs\n",
        "* Use the formula: **out_dim = [(input_dim - kernel_size + 2*padding)/stride] + 1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nCVPFp59Keq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)#in_channels, out_channels, kernel_size, \n",
        "        self.pool = nn.MaxPool2d(2,2)#2pool window of size 2, stride=2\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)#in_channels, out_channels, kernel_size, \n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)#in_features, out-features\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)#Flattening of tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "cnnModel = CNNModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAHpqAXg_M7v",
        "colab_type": "text"
      },
      "source": [
        "**Defining loss function and optimization algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzjbqXmN_Uph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(cnnModel.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMCPNZOv_kqx",
        "colab_type": "code",
        "outputId": "23ad993e-d592-42cd-ee12-4365aa4479a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  \n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels = data\n",
        "    \n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = cnnModel(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if i % 200 == 199:    # print every 200 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 200))\n",
        "      running_loss = 0.0\n",
        "print('Finished Training.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   200] loss: 2.303\n",
            "[1,   400] loss: 2.301\n",
            "[2,   200] loss: 2.294\n",
            "[2,   400] loss: 2.278\n",
            "[3,   200] loss: 2.216\n",
            "[3,   400] loss: 2.166\n",
            "[4,   200] loss: 2.098\n",
            "[4,   400] loss: 2.062\n",
            "[5,   200] loss: 2.000\n",
            "[5,   400] loss: 1.961\n",
            "[6,   200] loss: 1.908\n",
            "[6,   400] loss: 1.880\n",
            "[7,   200] loss: 1.818\n",
            "[7,   400] loss: 1.775\n",
            "[8,   200] loss: 1.713\n",
            "[8,   400] loss: 1.679\n",
            "[9,   200] loss: 1.630\n",
            "[9,   400] loss: 1.616\n",
            "[10,   200] loss: 1.579\n",
            "[10,   400] loss: 1.563\n",
            "Finished Training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uER4vTFA1Zm",
        "colab_type": "code",
        "outputId": "3c3b5378-5aef-412e-c093-9952b090aaf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "cnnModel.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(testloader))\n",
        "batch_size = images.shape[0] #100 is batch size\n",
        "print(images.size())\n",
        "output = cnnModel.forward(images)#classification probabilities based on untrained model\n",
        "plt.imshow(images[1].numpy().transpose(1,2,0))\n",
        "print('Predicted Label:',classes[np.argmax(output[1].data.numpy())])\n",
        "print('Actual Label:', classes[labels[1].numpy()])#actual label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 3, 32, 32])\n",
            "Predicted Label: deer\n",
            "Actual Label: dog\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7D8HlhrH3VU",
        "colab_type": "text"
      },
      "source": [
        "## Challenge 3.1:\n",
        "Add a third convolution layer (input channel =16, output channel =16 and kernel size =5) and apply maxpooling (pooling window 2*2) on newly added layer. Retrain the network and make the predictons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkgV6fXbIMxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make changes in this cell\n",
        "class CNNModel2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)#in_channels, out_channels, kernel_size, \n",
        "        self.pool = nn.MaxPool2d(2, 2)#pool window size=2, stride=2 \n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)#in_channels, out_channels, kernel_size,\n",
        "        self.conv3 = nn.Conv2d('Fix Me', 'Fix Me', 'Fix Me')#in_channels=16, out_channels=15, kernel_size=5\n",
        "        self.fc1 = nn.Linear('Fix Me', 'Fix Me')#in_features=16*24*24, out-features=120\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = 'Fix Me'#first x will be passed to self.conv3() then to F.relu then to self.pool. See above statement\n",
        "        x = x.view('Fix Me', 'Fix Me')#Flattening of tensor => (-1, 16* 24* 24)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "cnnModel2 = CNNModel2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pjQDKqXYHfT",
        "colab_type": "text"
      },
      "source": [
        "Training the model will require considerable amount of time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK18gZWlKdvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make No Changes In This Cell\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(cnnModel2.parameters(), lr=0.01)\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  \n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels = data\n",
        "    \n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = cnnModel2(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if i % 200 == 199:    # print every 200 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 200))\n",
        "      running_loss = 0.0\n",
        "print('Finished Training.')\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "cnnModel.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(testloader))\n",
        "batch_size = images.shape[0] #100 is batch size\n",
        "print(images.size())\n",
        "output = cnnModel.forward(images)#classification probabilities based on untrained model\n",
        "plt.imshow(images[1].numpy().transpose(1,2,0))\n",
        "print('Predicted Label:',classes[np.argmax(output[1].data.numpy())])\n",
        "print('Actual Label:', classes[labels[1].numpy()])#actual label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8VKmqWPVTEe",
        "colab_type": "text"
      },
      "source": [
        "# 4. Validation and Avoiding Overfitting\n",
        "\n",
        "\n",
        "*   Unlike Tensorflow 2.x, PyTorch does not give us accuracy of our model automatically\n",
        "* We need to write code to specify how to compute accuracy for our model\n",
        "* We need to test accuracy for both our test data and train data\n",
        "* If accuracy is too high for train data and too low for test data then we are are encountering **overfitting**\n",
        "* Then we will have to apply overfitting solutions like increasing dataset, modifying model, dropout, regularization etc.\n",
        "  \n",
        "Creating a validation function to calculate Accuracy of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1RJhGVt8208",
        "colab_type": "code",
        "outputId": "2b2d304f-d272-461c-9a24-9aaab59af4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Implement a function for the validation pass\n",
        "def validation(model, testloader, criterion):\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    for images, labels in testloader:\n",
        "\n",
        "        images.resize_(images.shape[0], 784)\n",
        "\n",
        "        output = model.forward(images)\n",
        "        test_loss += criterion(output, labels).item()#item() gives us the scalar value held in the tensor\n",
        "\n",
        "        equality = (labels.data == output.max(dim=1)[1])#remember output is in shape [64,10]\n",
        "        accuracy += equality.type(torch.FloatTensor).mean()\n",
        "    \n",
        "    return test_loss, accuracy\n",
        "  \n",
        "loss,accuracy=validation(model,testloader,lossCriteria)\n",
        "print(\"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEds2onoQjnd",
        "colab_type": "text"
      },
      "source": [
        "* Let us integrate above function in our training process to see real time accuracy of our model during training\n",
        "* Follwing cells show how to integrate our just defined validation function with training process of neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-O1Msu2JH49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unchanged Code of MNIST Classification discussed previously\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5, ), (0.5,)),# Subtract 0.5 from each pixel value and then divide it by 0.5 so as convert range of pixel value form -1 to +1\n",
        "                             ])\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
        "\n",
        "class Network(nn.Module):#specifying our network architecture\n",
        "  def __init__(self): #initializing the network\n",
        "    super().__init__()#it will call the init method of nn.Module\n",
        "    self.fc1 = nn.Linear(784,128)\n",
        "    self.fc2 = nn.Linear(128,64)\n",
        "    self.fc3 = nn.Linear(64,10)\n",
        "  \n",
        "  #all PyTorch Network must have forward method representing forward pass\n",
        "  def forward(self,x):#x is a tensor passed to forward function\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.softmax(x, dim=1)#The tensor will be of size 64(batcg size) * 10; we want to apply softmax to secomd dimension\n",
        "    return x   \n",
        "model = Network()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXvHUQ0QTW80",
        "colab_type": "code",
        "outputId": "50f871e0-ea05-471f-bd80-d8d1e60b4f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        }
      },
      "source": [
        "#Changes will be made in this cell\n",
        "lossCriteria = nn.CrossEntropyLoss() #defining loss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) #lr is learning rate \n",
        "epochs = 10\n",
        "print_every = 500 #to print the current loss after every 500 steps\n",
        "steps = 0\n",
        "for e in range(epochs):\n",
        "    running_loss = 0 #total loss till in previous 'print_every'steps\n",
        "    for images, labels in iter(trainloader):\n",
        "        steps += 1\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images.resize_(images.shape[0], 784)\n",
        "        \n",
        "        optimizer.zero_grad()#!!!VERY IMPORTANT. To ensure for each backpropagation, new gradients are computed\n",
        "        \n",
        "        # Forward and backward passes\n",
        "        output = model.forward(images)\n",
        "        loss = lossCriteria(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()#update weights\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if steps % print_every == 0:\n",
        "            \n",
        "            #Checking accuracy and loss\n",
        "            with torch.no_grad():#to speed up the process\n",
        "                test_loss, accuracy = validation(model, testloader, lossCriteria)#to test accuracy of model on train data\n",
        "                \n",
        "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
        "                  \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),#display test loss\n",
        "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))#display test accuracy\n",
        "            running_loss=0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10..  Training Loss: 2.298..  Test Loss: 2.292..  Test Accuracy: 0.259\n",
            "Epoch: 2/10..  Training Loss: 0.279..  Test Loss: 2.242..  Test Accuracy: 0.214\n",
            "Epoch: 2/10..  Training Loss: 2.173..  Test Loss: 2.112..  Test Accuracy: 0.319\n",
            "Epoch: 3/10..  Training Loss: 0.494..  Test Loss: 1.966..  Test Accuracy: 0.574\n",
            "Epoch: 3/10..  Training Loss: 1.906..  Test Loss: 1.858..  Test Accuracy: 0.649\n",
            "Epoch: 4/10..  Training Loss: 0.674..  Test Loss: 1.797..  Test Accuracy: 0.716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-df980d154202>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m#total loss till in previous 'print_every'steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Flatten MNIST images into a 784 long vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \"\"\"\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHzxPCv9Vl8d",
        "colab_type": "text"
      },
      "source": [
        "## Challenge 4.1\n",
        "Find out the final  test accuracy and test loss of the models you created in Challenge 2.1 and 2.2 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlESZ8xBWdw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for model created in Challenge 2.1\n",
        "loss2_1,accuracy2_1=validation('Replace With Model (Remove single quotes)','Replace With Test Loader(Remove single quotes)','Replace With Loss Criteria (Remove single quotes)')\n",
        "print(\"Test Loss:  {:.3f}.. \".format(loss2_1/len('Replace With Test Loader(Remove single quotes)'))\n",
        "print(\"Test Accuracy: {:.3f}\".format(accuracy2_1/len('Replace With Test Loader(Remove single quotes)')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cU4Uz4XXlKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for model created in Challenge 2.2\n",
        "loss2_2,accuracy2_2=validation('Replace With Model (Remove single quotes)','Replace With Test Loader(Remove single quotes)','Replace With Loss Criteria (Remove single quotes)')\n",
        "print(\"Test Loss: {:.3f}.. \".format(loss2_2/len('Replace With Test Loader(Remove single quotes)'))\n",
        "print(\"Test Accuracy: {:.3f}\".format(accuracy2_2/len('Replace With Test Loader(Remove single quotes)')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfMecQx2d9vi",
        "colab_type": "text"
      },
      "source": [
        "# 5. Saving and Loading Trained Networks in PyTorch\n",
        "\n",
        "Saving the model involves two things\n",
        "\n",
        "*   Saving the state of the model (weights and bias). \n",
        "* This is is useful when we dont want to retrain the model again and again\n",
        "* Instead we can reload the state of model in future\n",
        "* The parameters (weights, biases etc) are stored in model's **state_dict**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzW898_gjDOn",
        "colab_type": "code",
        "outputId": "c3d439c4-0bb0-4a44-d05b-593f7b95a4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print(\"Model under consideration: \",model)\n",
        "print(\"The keys of state dictionary \", model.state_dict().keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model under consideration:  Network(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "The keys of state dictionary  odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uNSZ3QIJ1UK",
        "colab_type": "code",
        "outputId": "5a7fe04d-56c5-4591-c66b-56bf1825d9b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2887
        }
      },
      "source": [
        "print(\"The values of state dictionary \", model.state_dict().values())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The values of state dictionary  odict_values([tensor([[ 0.0298, -0.0133,  0.0106,  ..., -0.0108,  0.0317, -0.0295],\n",
            "        [ 0.0292, -0.0115, -0.0317,  ..., -0.0301, -0.0112,  0.0167],\n",
            "        [ 0.0121, -0.0104,  0.0139,  ...,  0.0146,  0.0056,  0.0124],\n",
            "        ...,\n",
            "        [ 0.0300,  0.0006,  0.0294,  ..., -0.0380, -0.0193, -0.0354],\n",
            "        [-0.0328, -0.0055,  0.0167,  ..., -0.0281, -0.0148,  0.0199],\n",
            "        [ 0.0197,  0.0213,  0.0152,  ...,  0.0208, -0.0045, -0.0418]]), tensor([ 0.0195,  0.0327,  0.0059, -0.0263, -0.0234,  0.0191, -0.0033,  0.0256,\n",
            "         0.0069,  0.0090,  0.0069, -0.0319,  0.0266,  0.0136,  0.0357, -0.0198,\n",
            "         0.0073, -0.0281, -0.0208, -0.0270, -0.0118, -0.0062,  0.0077,  0.0018,\n",
            "        -0.0050, -0.0024,  0.0236,  0.0190,  0.0012, -0.0288, -0.0203,  0.0053,\n",
            "         0.0046,  0.0203, -0.0271,  0.0237,  0.0309,  0.0058,  0.0275, -0.0029,\n",
            "        -0.0167,  0.0131,  0.0225,  0.0047, -0.0166,  0.0117, -0.0123,  0.0009,\n",
            "        -0.0262, -0.0138, -0.0113, -0.0248, -0.0038,  0.0028, -0.0252, -0.0106,\n",
            "        -0.0007,  0.0289,  0.0237, -0.0220, -0.0159,  0.0066, -0.0015,  0.0103,\n",
            "        -0.0109,  0.0256,  0.0060,  0.0196, -0.0231,  0.0016,  0.0006,  0.0168,\n",
            "        -0.0304, -0.0171, -0.0103,  0.0051, -0.0141, -0.0097, -0.0109, -0.0186,\n",
            "         0.0028,  0.0376, -0.0249, -0.0248, -0.0011,  0.0163,  0.0142, -0.0273,\n",
            "        -0.0171,  0.0106, -0.0022, -0.0291,  0.0019,  0.0200, -0.0006, -0.0313,\n",
            "        -0.0213, -0.0079, -0.0320,  0.0115, -0.0012, -0.0263, -0.0236,  0.0173,\n",
            "         0.0129,  0.0157, -0.0044,  0.0360,  0.0299, -0.0219, -0.0290,  0.0185,\n",
            "        -0.0083, -0.0281, -0.0093, -0.0098,  0.0133,  0.0276, -0.0099,  0.0329,\n",
            "         0.0317, -0.0156, -0.0117,  0.0130,  0.0051, -0.0199, -0.0108, -0.0070]), tensor([[ 0.0509, -0.0904, -0.0210,  ..., -0.0646,  0.0863, -0.0134],\n",
            "        [ 0.0259,  0.0430,  0.0770,  ..., -0.0943,  0.0544, -0.0590],\n",
            "        [ 0.0641, -0.0088,  0.0644,  ..., -0.0486, -0.0453,  0.1048],\n",
            "        ...,\n",
            "        [-0.0032, -0.0645, -0.0231,  ...,  0.0745,  0.0787, -0.0045],\n",
            "        [ 0.0052, -0.0270, -0.0291,  ..., -0.0600,  0.0050,  0.0726],\n",
            "        [-0.0262,  0.0402,  0.0730,  ...,  0.0336,  0.0594, -0.0508]]), tensor([ 0.0115,  0.0596, -0.0488, -0.0090, -0.0166,  0.0403, -0.0631,  0.0923,\n",
            "        -0.0541,  0.0729, -0.0191, -0.0539, -0.0261,  0.0367,  0.0410,  0.0255,\n",
            "         0.0255,  0.0615,  0.0553, -0.0016,  0.0187,  0.0227,  0.0704, -0.0414,\n",
            "        -0.0721, -0.0041, -0.0356,  0.0158,  0.0144, -0.0299,  0.0138,  0.0861,\n",
            "         0.0277, -0.0530, -0.0741,  0.0955,  0.0796, -0.0507,  0.0685, -0.0798,\n",
            "        -0.0110,  0.0141,  0.0053,  0.0350, -0.0786,  0.0728, -0.0200,  0.0721,\n",
            "        -0.0297,  0.0151, -0.0516, -0.0363,  0.0661,  0.0143, -0.0654,  0.0267,\n",
            "         0.0445, -0.0199, -0.0426, -0.0133,  0.0939,  0.0315, -0.0330,  0.0654]), tensor([[-7.4066e-02,  1.5976e-01, -3.7108e-02,  1.0881e-01,  9.6027e-02,\n",
            "         -2.1563e-01, -8.9004e-02,  1.3953e-01, -4.1103e-02,  1.1252e-01,\n",
            "          3.2603e-03, -2.4052e-01,  1.4553e-02,  4.5802e-02, -1.7038e-01,\n",
            "         -2.7096e-02, -1.2008e-01,  1.5733e-01, -1.0357e-01,  2.7847e-03,\n",
            "          8.3932e-02,  2.1705e-01, -2.5683e-01, -1.2550e-01,  3.3571e-02,\n",
            "          9.9186e-02, -2.5280e-02,  9.9391e-02,  6.5589e-02, -9.0358e-02,\n",
            "         -4.1070e-02, -1.4188e-01,  3.1137e-01,  3.2493e-03, -1.4193e-01,\n",
            "          3.2255e-01,  8.5153e-02,  8.9318e-02,  1.5032e-01,  1.2837e-02,\n",
            "          2.7879e-02, -1.5082e-01, -8.3125e-02,  3.5677e-02,  9.1012e-02,\n",
            "          2.7542e-04,  8.4709e-02, -3.6466e-02,  4.0019e-02, -1.0304e-01,\n",
            "          2.1143e-01,  1.7287e-02,  4.4577e-02, -2.7554e-01,  2.3306e-01,\n",
            "          2.8473e-01, -7.8086e-02, -2.8568e-02, -1.5846e-01, -7.8995e-02,\n",
            "          1.1576e-01,  1.1463e-01, -6.1894e-02,  5.0920e-02],\n",
            "        [-4.4402e-02,  7.0506e-02, -2.3923e-01, -7.5598e-02, -9.6423e-02,\n",
            "          2.8996e-01, -5.4978e-02, -5.3974e-02,  1.1764e-01,  3.5075e-02,\n",
            "         -2.1465e-01, -4.4153e-02,  1.2041e-01,  1.8890e-02,  1.1269e-01,\n",
            "          2.2201e-01,  5.6640e-02, -1.9745e-01, -2.6279e-01, -6.0444e-02,\n",
            "          1.9990e-02, -2.6751e-01,  5.0493e-02, -1.2747e-01,  2.1917e-01,\n",
            "          2.5204e-02,  6.1156e-03, -4.8717e-02,  1.5928e-01, -8.7844e-02,\n",
            "         -1.1909e-01, -5.9263e-02, -6.4926e-02,  6.4823e-02,  3.3222e-01,\n",
            "         -3.1309e-01,  3.1846e-02,  9.4034e-03, -3.5936e-02,  1.1922e-02,\n",
            "         -1.7121e-01, -2.9545e-01, -2.4085e-02,  2.3824e-02, -8.9288e-02,\n",
            "          1.6280e-01,  1.0305e-02,  1.2930e-01, -1.1548e-01,  1.7368e-01,\n",
            "         -1.9843e-01, -9.6509e-02,  7.9834e-02,  1.8882e-01,  7.8578e-03,\n",
            "          8.4645e-02, -1.2017e-01,  6.0115e-02,  2.4839e-01, -2.8987e-02,\n",
            "          2.0852e-01,  8.6169e-02,  1.9545e-02, -6.4812e-02],\n",
            "        [-4.8234e-02, -1.8361e-01,  8.0270e-02, -1.2632e-01,  9.4054e-02,\n",
            "          4.4858e-02,  2.0078e-01,  1.0017e-01,  4.5004e-02, -1.1678e-02,\n",
            "          1.5214e-01, -1.5946e-01, -3.4938e-03,  1.4758e-01,  6.5489e-02,\n",
            "          5.5511e-03,  1.8025e-01, -1.0593e-01,  2.3038e-01,  1.4810e-02,\n",
            "          2.1184e-02, -2.9567e-02,  1.4264e-01, -5.7597e-02, -1.6789e-01,\n",
            "         -2.1669e-04, -5.7912e-02, -1.2466e-01,  1.1984e-01, -4.0768e-02,\n",
            "          1.6614e-03, -2.3958e-01,  1.8448e-01,  1.1658e-01, -4.6871e-02,\n",
            "         -9.3433e-02, -9.7749e-04,  1.0652e-01, -8.6443e-02,  6.5040e-02,\n",
            "         -2.1815e-01, -1.7637e-01,  1.1671e-01, -1.5879e-01,  9.3319e-02,\n",
            "         -1.0606e-01,  6.4822e-02, -9.5808e-02, -7.8263e-02,  7.6396e-02,\n",
            "          1.2550e-01, -8.8793e-02,  2.6377e-01,  2.6639e-02, -2.5016e-02,\n",
            "          5.6602e-02,  1.6885e-01,  6.7798e-02, -2.8700e-02,  5.8976e-02,\n",
            "         -6.1852e-02, -1.6951e-01,  4.6903e-02,  1.5557e-01],\n",
            "        [ 2.8107e-01, -5.0052e-02,  1.1374e-01,  1.0904e-01,  1.4206e-02,\n",
            "         -9.6578e-02, -6.3525e-02,  1.3196e-01,  5.8521e-02,  1.8103e-01,\n",
            "          9.0322e-03, -9.2052e-02,  2.7417e-01,  4.9741e-02,  1.5472e-02,\n",
            "         -2.9310e-03,  2.3462e-01, -8.0577e-02,  1.3180e-01, -8.2308e-02,\n",
            "          3.9837e-02, -1.7656e-01, -5.2098e-02, -2.2209e-01,  2.6364e-02,\n",
            "          3.2953e-02, -1.0314e-01,  8.6826e-03, -4.8426e-02, -9.4544e-03,\n",
            "         -1.7644e-01,  6.8035e-02, -1.2629e-01, -1.3815e-01, -1.0115e-02,\n",
            "          2.2479e-01, -6.2024e-02, -8.9397e-02,  1.8225e-01,  8.4867e-02,\n",
            "          2.3678e-01, -1.8044e-01,  8.3486e-02, -4.0623e-02,  1.0947e-01,\n",
            "         -8.2087e-02,  5.0319e-04, -6.8059e-02, -8.2679e-02, -7.3964e-03,\n",
            "         -8.8166e-03,  1.7354e-03, -4.7180e-02,  1.8694e-01, -9.3969e-02,\n",
            "          2.1679e-01, -1.1497e-01, -9.4086e-02, -4.4650e-02,  1.9611e-01,\n",
            "         -1.8927e-01, -1.4039e-01, -1.5921e-02, -1.3177e-01],\n",
            "        [ 2.8536e-02,  1.1919e-01, -4.3050e-02, -1.2477e-01, -7.6566e-02,\n",
            "         -1.9030e-01,  1.3596e-01,  4.5834e-02, -1.6271e-03, -3.0375e-02,\n",
            "         -1.6181e-02,  2.1371e-01, -2.7327e-01, -4.1403e-02,  1.2435e-01,\n",
            "         -7.9980e-02, -2.0789e-01, -7.9176e-02,  2.7675e-02, -8.5445e-02,\n",
            "         -1.7257e-02,  1.7864e-01, -1.3372e-01,  1.6008e-01, -1.2201e-01,\n",
            "         -6.2979e-02, -1.4465e-01, -8.1015e-02,  1.7538e-01, -6.9879e-02,\n",
            "          1.0424e-01,  2.7305e-01, -1.1240e-01,  1.0685e-01, -2.7577e-02,\n",
            "         -2.1379e-01,  3.7783e-02, -4.5298e-02,  8.9908e-02,  2.4907e-02,\n",
            "         -7.0042e-02,  3.3940e-01,  2.2367e-02,  7.5095e-02, -1.8647e-02,\n",
            "         -2.3378e-02,  2.4592e-02,  2.9937e-02, -2.3064e-02,  1.3631e-01,\n",
            "         -1.9989e-01, -1.1326e-01,  2.1996e-02, -4.6490e-02, -1.6042e-01,\n",
            "         -1.7935e-01,  1.5413e-01, -8.2981e-02, -1.3146e-01,  9.8506e-02,\n",
            "          1.1237e-01,  6.1724e-02, -8.6741e-02, -9.4021e-02],\n",
            "        [ 1.1461e-01, -1.4063e-01,  4.9531e-02,  1.0637e-01,  6.8909e-02,\n",
            "          3.3758e-03,  3.0236e-02, -2.4818e-02, -5.2703e-02,  1.0204e-01,\n",
            "         -2.3959e-02, -6.7853e-03,  1.0168e-01,  8.2678e-02, -8.5888e-02,\n",
            "          1.6365e-02,  5.6333e-02, -1.1998e-01, -1.3224e-02,  5.2553e-02,\n",
            "         -3.9424e-02, -2.4502e-03, -7.7769e-02,  1.0509e-01, -8.0265e-02,\n",
            "         -4.6436e-02,  1.9622e-02, -1.7603e-02,  8.6021e-02,  9.3620e-02,\n",
            "          1.1351e-01, -8.5002e-02,  4.6660e-02, -2.8778e-02, -1.2036e-01,\n",
            "         -1.9866e-02, -7.6254e-02, -1.1429e-01, -1.2595e-03,  5.3334e-02,\n",
            "         -7.6219e-02, -1.3091e-01, -1.2475e-02, -1.1395e-01, -1.4374e-02,\n",
            "         -7.2860e-03, -2.0425e-02,  2.0042e-02,  3.8168e-02, -7.6739e-02,\n",
            "         -1.0259e-01,  5.9596e-02, -1.3310e-01,  1.7637e-02, -3.9365e-02,\n",
            "         -1.1786e-01,  2.8874e-03,  8.3538e-02,  4.4453e-02,  1.0345e-02,\n",
            "         -1.4167e-01, -2.5389e-02,  8.7255e-02,  8.2227e-02],\n",
            "        [-1.7162e-01, -1.6912e-01, -4.2462e-02,  1.8487e-01,  1.1378e-01,\n",
            "          2.1417e-01, -2.1883e-01,  8.2504e-02,  8.3426e-02, -1.0414e-01,\n",
            "          1.7364e-01,  7.5524e-02, -1.6024e-01,  2.1305e-01,  6.7736e-02,\n",
            "         -9.4940e-02,  1.8907e-02,  2.5546e-01,  6.1138e-02,  6.2866e-02,\n",
            "         -1.0745e-01, -8.2507e-02,  1.7742e-01,  2.6254e-01,  1.2019e-01,\n",
            "          1.4755e-01, -1.7329e-01,  9.3526e-02,  1.3948e-01,  5.4862e-02,\n",
            "         -6.2635e-02, -2.0338e-01,  1.9806e-01, -6.3354e-02,  6.3911e-02,\n",
            "         -1.8477e-02, -1.2248e-01, -1.0569e-01, -8.1098e-02, -1.2968e-03,\n",
            "         -1.0352e-01,  7.1748e-02,  8.2814e-02, -5.7561e-02,  6.5332e-02,\n",
            "          7.4621e-02, -5.8159e-02, -1.6650e-02,  7.8418e-02,  2.3737e-01,\n",
            "         -1.2084e-01,  1.8328e-01, -1.5833e-01, -2.0469e-01, -1.2837e-01,\n",
            "         -1.9536e-01,  2.9017e-02, -6.6123e-02, -1.3885e-01, -2.3498e-01,\n",
            "         -1.5226e-03, -2.9287e-02, -2.8715e-02,  5.7196e-02],\n",
            "        [ 1.4583e-01, -8.3254e-02, -4.5481e-02,  8.5602e-02, -8.0286e-02,\n",
            "         -1.0522e-01,  1.2758e-01,  8.6418e-02,  1.1969e-01,  3.5087e-02,\n",
            "         -1.3393e-01,  1.6845e-01,  1.9026e-01, -6.3905e-02, -2.1322e-01,\n",
            "         -4.2191e-02, -1.3441e-01, -1.3976e-02, -1.7830e-01,  9.0836e-02,\n",
            "          1.8563e-01,  2.4284e-01, -2.1076e-01,  9.0201e-02,  8.9823e-02,\n",
            "          4.0451e-02,  1.9838e-01,  1.4266e-01, -1.4943e-01,  1.6704e-01,\n",
            "          4.0975e-02, -2.8498e-02, -1.0102e-01,  1.3021e-01,  1.2179e-02,\n",
            "          4.2250e-02, -1.0170e-01,  6.5058e-03, -1.4374e-01,  1.3784e-02,\n",
            "          1.7727e-01,  2.9859e-01, -8.3347e-02, -1.5053e-01,  1.0520e-01,\n",
            "         -4.2374e-03,  1.9768e-02,  6.7514e-02, -1.0041e-01, -2.9308e-01,\n",
            "         -1.5802e-01,  5.2137e-02,  1.5446e-01,  1.2031e-01, -6.7496e-02,\n",
            "         -4.6363e-02, -1.4604e-01, -5.8719e-02,  2.1278e-01, -1.3297e-02,\n",
            "          9.5013e-02,  2.5268e-01,  5.6534e-02, -1.1849e-01],\n",
            "        [ 3.8325e-02, -1.3745e-01,  1.1994e-02,  5.2087e-02,  4.2763e-02,\n",
            "          1.4132e-01,  1.8391e-01, -1.4940e-01, -7.6514e-03, -6.7981e-02,\n",
            "         -2.7846e-02,  7.6610e-02, -1.2323e-01, -5.0960e-02,  9.6777e-02,\n",
            "          6.3905e-02, -4.3414e-02, -5.5635e-03, -1.5622e-02, -1.0629e-01,\n",
            "          4.7191e-02, -8.9585e-02,  3.3408e-02, -6.6814e-02,  8.0962e-02,\n",
            "          5.2802e-03, -2.1952e-02, -3.9018e-03,  5.7285e-03,  8.9691e-02,\n",
            "          4.1465e-02,  6.6123e-02,  1.0808e-01,  5.8691e-02, -1.6800e-01,\n",
            "          1.0625e-01,  9.3567e-02,  3.4375e-02, -1.1716e-01,  1.0910e-01,\n",
            "          6.7341e-02,  3.2240e-02, -6.9952e-02,  1.4487e-02, -5.4723e-02,\n",
            "          8.9161e-02, -7.2558e-02, -1.2339e-01,  2.8378e-02,  4.8851e-02,\n",
            "          1.9129e-01,  2.5299e-02, -1.5244e-01, -2.7855e-02,  7.2913e-02,\n",
            "          1.4048e-01,  1.6636e-01, -3.8778e-02, -7.2752e-02,  1.8723e-01,\n",
            "         -4.7537e-02, -1.2710e-01,  9.6838e-02, -1.1285e-01],\n",
            "        [ 1.0930e-01,  7.7883e-02,  4.2258e-02,  6.8787e-03, -1.1652e-01,\n",
            "          5.3441e-02, -3.7445e-02,  3.1972e-03, -1.0329e-01, -1.2758e-01,\n",
            "          1.0590e-02,  9.6046e-02,  6.7217e-03, -2.6699e-02, -3.0866e-02,\n",
            "         -1.1153e-01, -7.2315e-02,  9.7577e-02, -1.4109e-01, -2.6410e-02,\n",
            "          6.0863e-02, -1.0213e-01, -5.1212e-02,  4.7000e-02,  8.0123e-02,\n",
            "          9.5334e-02,  1.1196e-03,  5.9131e-03,  9.0796e-02,  4.9142e-02,\n",
            "         -6.0935e-02,  1.0553e-01, -5.5192e-02,  1.2191e-01, -1.1961e-01,\n",
            "          1.6567e-02, -2.0130e-02, -9.4472e-02, -5.8824e-02,  2.6704e-02,\n",
            "         -1.0164e-01,  1.1115e-01,  2.3111e-02, -1.4436e-01,  1.8518e-02,\n",
            "         -6.5820e-04,  3.9644e-02, -7.3051e-02,  5.2774e-02, -8.9473e-02,\n",
            "         -6.2883e-02, -4.3524e-02, -5.3329e-02, -5.8662e-03, -1.0724e-01,\n",
            "         -1.4800e-01,  7.4321e-02,  8.7878e-02, -1.5163e-02,  1.6733e-02,\n",
            "         -2.5473e-02, -1.0560e-01, -1.1781e-01, -7.3698e-02]]), tensor([ 0.1128, -0.0548,  0.1088, -0.0588,  0.0557, -0.1070, -0.0437, -0.1170,\n",
            "        -0.0925, -0.0221])])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Ok7Bk8KuvZ",
        "colab_type": "text"
      },
      "source": [
        "**Saving the state_dict():**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNYK_ui5IrS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmt4rNOyLDi-",
        "colab_type": "text"
      },
      "source": [
        "Ensure that the state of model is actually saved by selecting left menu then selecting File and then selecting refresh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2UxSFjqLtFN",
        "colab_type": "text"
      },
      "source": [
        "**Loading the pre trained model using saved_dict:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAxL5jGnLBBF",
        "colab_type": "code",
        "outputId": "2bf56dd8-71b2-42b5-de4b-5980f17a9c74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "state_dict = torch.load('checkpoint.pth')\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7EB4s0zL5sL",
        "colab_type": "code",
        "outputId": "4990e666-3341-4db4-a907-1f4d61001f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model_reloaded=Network() #create a new model\n",
        "model_reloaded.load_state_dict(state_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjilOGfy-tNS",
        "colab_type": "text"
      },
      "source": [
        "Ignore if you see a message 'IncompatibleKeys(missing_keys=[], unexpected_keys=[])'. \n",
        "\n",
        "**Making predictions with the newly model with pretrained wieghts and biases:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzi6UbJzM0YY",
        "colab_type": "code",
        "outputId": "1438a543-ed4c-4154-d8f7-d9c8fdb8a1d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "model_reloaded.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(trainloader))\n",
        "batch_size = images.shape[0] #64 is batch size\n",
        "images.resize_(batch_size, 784)#in_place resizing of image\n",
        "print(images.size())\n",
        "output = model_reloaded.forward(images)#classification probabilities based on untrained model\n",
        "print('Predicted Label:',np.argmax(output[1].data.numpy()))\n",
        "print('Actual Label:', labels[1].numpy())#actual label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 784])\n",
            "Predicted Label: 2\n",
            "Actual Label: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeQMuO3GALRs",
        "colab_type": "text"
      },
      "source": [
        "## Challenge 5.1\n",
        "Save the model that you created in Challenge 2.1 and reload the model and make the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn8CdtNkAbbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pE2Opo0_-XZ",
        "colab_type": "text"
      },
      "source": [
        "# 6. Transfer Learning Using PyTorch\n",
        "\n",
        "\n",
        "*   We will modify a pre-trained model to classify images as cats or dogs\n",
        "*   There are various pre-trained models available in PyTorch via **[torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html)** that are already trained on massive datasets such as ImageNet\n",
        "* The pre-trained models are good feature detectors that can be used as an input for simple feed forward classifiers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgWmi-WqjQIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#An additional import is required\n",
        "from torchvision import models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjBFveOzPYzI",
        "colab_type": "text"
      },
      "source": [
        "We will use transfer learning to distinguish beteen images of Dogs and Cats\n",
        "\n",
        "Download Cat Vs Dog Dataset :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAWbyn_OPZI-",
        "colab_type": "code",
        "outputId": "4390ef20-9683-48b1-9125-f5af74c43e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "!wget http://files.fast.ai/data/dogscats.zip\n",
        "!unzip -q dogscats.zip "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-22 07:26:51--  http://files.fast.ai/data/dogscats.zip\n",
            "Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n",
            "Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 857214334 (818M) [application/zip]\n",
            "Saving to: ‘dogscats.zip.1’\n",
            "\n",
            "dogscats.zip.1      100%[===================>] 817.50M  21.1MB/s    in 63s     \n",
            "\n",
            "2019-05-22 07:27:53 (13.1 MB/s) - ‘dogscats.zip.1’ saved [857214334/857214334]\n",
            "\n",
            "replace dogscats/sample/train/cats/cat.2921.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7a14HMoEHJo",
        "colab_type": "text"
      },
      "source": [
        "The pretrained model that we will be using is **[DenseNet](https://pytorch.org/docs/stable/torchvision/models.html#id5)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J1x_ifREVM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.densenet121(pretrained=True)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKTc_NkGDB5j",
        "colab_type": "text"
      },
      "source": [
        "**The input that the network expects:**\n",
        "\n",
        "*   All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W)\n",
        "* Here H and W are expected to be at least 224\n",
        "*   The images have to be loaded in to a range of [0, 1] \n",
        "* Then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AqHFhloCcr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'dogscats'\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])])\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=transform)\n",
        "test_data = datasets.ImageFolder(data_dir + '/valid', transform=transform)\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=64,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWXW3vokEoKv",
        "colab_type": "text"
      },
      "source": [
        "In Transfer Learning, normally the model has two parts:\n",
        "\n",
        "\n",
        "*   **Features Part:** Where the model extract the features of the data. This part normally remains unchanged\n",
        "*   **Classifier Part:** Here the model performs classification. This part needs to be updated as per the given problem\n",
        "\n",
        "We need to train only the updated classifier part on our dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2ybg2bBIN8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False#this ensures that model will not get trained during training\n",
        "\n",
        "from collections import OrderedDict\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(1024, 500)),#The first value must be same as output of classifier\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('fc2', nn.Linear(500, 2)),\n",
        "                          ('output', nn.Softmax(dim=1))\n",
        "                          ]))\n",
        "    \n",
        "model.classifier = classifier #This updates the classifier part of the model with newly created classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGca1ibDSc_w",
        "colab_type": "text"
      },
      "source": [
        "Training classifier part of our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdizsf8FSbLu",
        "colab_type": "code",
        "outputId": "66bbe066-b7f5-4c67-ec6c-82e9cf9a9e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import time\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Only train the classifier parameters, feature parameters are frozen\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "for ii, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        outputs = model.forward(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()#update weights\n",
        "\n",
        "        if ii==3:#only training on 3 batches\n",
        "            break\n",
        "print(\"Training on CPU takes: {}\".format((time.time()-start)/3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on CPU takes: 6.0631247361501055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTEVrrvDKI0B",
        "colab_type": "text"
      },
      "source": [
        "* For training our transfer learning based  model we also  can **levarage the power of GPU**:\n",
        "* Training our model on GPU results in significant decrease in training time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59U9d2R6VWGb",
        "colab_type": "code",
        "outputId": "9dc46d34-ed7c-46f8-c9f5-ac2775e4540b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Only train the classifier parameters, feature parameters are frozen\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "model.to(device)#moving model to GPU (if available)\n",
        "\n",
        "for ii, (inputs, labels) in enumerate(trainloader):\n",
        "  # Move input and label tensors to the GPU \n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  outputs = model.forward(inputs)\n",
        "  loss = criterion(outputs, labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()#update weights\n",
        "\n",
        "  if ii==3:#only training on 3 batches\n",
        "    break\n",
        "print(f\"Device = {device}; Time per batch: {(time.time() - start)/3:.3f} seconds\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device = cuda:0; Time per batch: 0.009 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4_uC3fCZ7NJ",
        "colab_type": "text"
      },
      "source": [
        "Doing Classification with our transfer learning based model for Cats Vs Dogs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sodIL5ZQDlyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(testloader))\n",
        "print(images.size())\n",
        "output = model.forward(images)\n",
        "plt.imshow(images[50].numpy().transpose(1,2,0))\n",
        "print('Predicted Label:',np.argmax(output[50].data.numpy()))#1 is dog 0 is cat\n",
        "print('Actual Label:', labels[50].numpy())#actual label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSTjY6a3k2kA",
        "colab_type": "text"
      },
      "source": [
        "## Solution: Challenge 2.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEAR2S4Pk-UO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load dataset in this cell\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5, ), (0.5,)) # Subtract 0.5 from each pixel value and then divide it by 0.5 so as convert range of pixel value form -1 to +1\n",
        "                             ])\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('FashionMNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST('FashionMNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eodrgqAElX93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a feed-forward network\n",
        "#Fix Me - Add layers here. Take help from above cells\n",
        "input_size=784\n",
        "output_size=10\n",
        "model4 = nn.Sequential(nn.Linear(input_size, 400),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(400, 200),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(200, 100),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(100,output_size),\n",
        "                      nn.Softmax(dim=1))\n",
        "print(model4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLtNbfyQlhKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Specify loss and optimizer\n",
        "lossCriteria = nn.CrossEntropyLoss() #defining loss\n",
        "optimizer = torch.optim.SGD(model4.parameters(), lr=0.005) #lr is learning rate "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73jjc30Hlnf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train network in this cell\n",
        "train_loss=[]\n",
        "epochs = 10\n",
        "print_every = 40 #to print the current loss after every 40 steps\n",
        "steps = 0\n",
        "for e in range(epochs):\n",
        "    running_loss = 0 #total loss till in previous 'print_every'steps\n",
        "    for images, labels in iter(trainloader):\n",
        "        steps += 1\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images.resize_(images.shape[0], 784)\n",
        "        \n",
        "        optimizer.zero_grad()#!!!VERY IMPORTANT. To ensure for each backpropagation, new gradients are computed\n",
        "        \n",
        "        # Forward and backward passes\n",
        "        output = model4.forward(images)\n",
        "        loss = lossCriteria(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()#update weights\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        train_loss.append(loss)\n",
        "        if steps % print_every == 0:\n",
        "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
        "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
        "            \n",
        "            running_loss = 0\n",
        "plt.plot(train_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAD7kqbFl1XC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make prediction(s) in this cell\n",
        "#Do not change anything in this cell\n",
        "#Testing predictions of trained model\n",
        "model4.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(testloader))\n",
        "batch_size = images.shape[0] #64 is batch size\n",
        "images.resize_(batch_size, 784)#in_place resizing of image\n",
        "print(images.size())\n",
        "prob = model4.forward(images)#classification probabilities based on untrained model\n",
        "print('Predicted Label:',np.argmax(prob[1].data.numpy()))\n",
        "print('Actual Label:', labels[1].numpy())#actual label"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}